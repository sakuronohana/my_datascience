{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS-Glossar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakuronohana/my_datascience/blob/master/DS_Glossar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8UD6-z7J8Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jUrFBzk0JBn",
        "colab_type": "text"
      },
      "source": [
        "#A#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd2u7UHGVKr6",
        "colab_type": "text"
      },
      "source": [
        "##Active Learning##\n",
        "Active learning (machine learning) is a specialized version of semi-supervised learning. Here, you use the labelled samples, and among the unlabelled samples, you try to find out labelling which small number of them will get much better performance. It is an iterative algorithm where the learner requests the user to label a few samples in each iteration, and this is repeated till convergence. This way, only a small amount of unlabelled data needs to be labelled to get very good performance.  \n",
        "\n",
        "![alt text](https://image.slidesharecdn.com/voropaev-130106160851-phpapp02/95/introduction-to-active-learning-11-1024.jpg?cb=1357488619)\n",
        "\n",
        "[DSC Webinar to the topic Acitve Learning](https://vimeo.com/304462110)\n",
        "\n",
        "[More informations (PDF)](http://burrsettles.com/pub/settles.activelearning.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd8A7QCaNyh3",
        "colab_type": "text"
      },
      "source": [
        "#B#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M36HCk7EnuKC",
        "colab_type": "text"
      },
      "source": [
        "##Batch##\n",
        "It is not possible ot pass the entire dataset (Epoch) into the neural net at once. So, you divide dataset into Number of Batches or sets or parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwxlgK6Aova-",
        "colab_type": "text"
      },
      "source": [
        "##Batch Size##\n",
        "A batch size representing a total number of training examples in a single batch (see Batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_zL8aN0N2Ug",
        "colab_type": "text"
      },
      "source": [
        "##Bias##\n",
        "The bias sometimes also called y-intercept, is a value which defines the threshold withhin a neuron. It belongs to the so called transfer function which sums all inputs (x) and weights for a neuron and finally adds the bias.\n",
        "\n",
        "$ f(x)  = (x_i * w_i) + b$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isb2boDgMbq_",
        "colab_type": "text"
      },
      "source": [
        "#C#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mgCKxaTMgVb",
        "colab_type": "text"
      },
      "source": [
        "##Cost function## \n",
        "Also called **Loss function**\n",
        "Is an algorithm which lets us find weights and biases so that the output from the network approximates y(x) for all training inputs x. To quantify how well we're achieving this goal we define a cost function. We need a function that will minimize the parameters over our dataset. One common function that is often used is **mean squared error** (MSE), which measure the difference between the estimator (the dataset) and the estimated value (the prediction)\n",
        "\n",
        "MSE Notation:\n",
        "\n",
        "$MSE = \\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK1kZuaY5h6_",
        "colab_type": "text"
      },
      "source": [
        "##Classification## \n",
        "Classification is the process of predicting the class (e.g spam or not spam) of given data points and belongs to Supervised Learning. Classes are sometimes called as targets/ labels or categories. Classification predictive modeling is the task of approximating a mapping **function (f)** from **input variables (X)** to discrete **output variables (y)**. \n",
        "There existing two kinds fo Classifications:\n",
        "\n",
        "\n",
        "1.   Linear Classification\n",
        "2.   non-linear Classification\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://data-science-blog.com/wp-content/uploads/2017/12/klassifikation-regression-machine-learning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCGeGWcdDDC4",
        "colab_type": "text"
      },
      "source": [
        "##Cross Validation##\n",
        "\n",
        "Cross-validation, sometimes called rotation estimation, or out-of-sample testing is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
        "\n",
        "![alt text](https://st1.ning.com/topology/rest/1.0/file/get/2664616872?profile=RESIZE_710x)\n",
        "\n",
        "Artikel: https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8them1n0Mev",
        "colab_type": "text"
      },
      "source": [
        "#D#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnv14Q1h7omR",
        "colab_type": "text"
      },
      "source": [
        "##Distributed Training##\n",
        "\n",
        "Due to performance issues on a single machine, the training of a neural network model can take hours until days depending on the size of the epoch (trainig data). An option to speed up the training is the use of GPU/TBU but if this is still not fast enough there is another way called Distributed Training. With this technic you will be able to use one machine with multiple devices (GPU/TBU) or multiple machines with multiple devices.\n",
        "The most commen used DT architecture is called **Data parallelism** in which the input datas will be split up in several subsets. The amount of subsets depends on the amount of **Worker** available. Each worker computes the loss and the gradients on the same model, where the gradients are used to update the parameters of the model.\n",
        "There're two main approaches to use this gradients:\n",
        "\n",
        "1.   **Async Parameter Server** - The worker fetch the parameter from the Param Servers (PS) und computes independently the loss and gradient. Afterwards the worker sends the gradients back to the PSs which updates the models parameters. \n",
        "2.   **Sync Allreduce** - All workers in this archtecure holding a copy of parameters. Once  the loss and gradients has been computed the workers propagate the gradients to each other and every worker update their copy of the  models parameters. All worker are synchronised and the next round of computation doesn't begin until all workers are updated.\n",
        "\n",
        "There is also another DT architecture called **Model parallelism**. This one is used, when the model doesn't fit in the memory of one device. So the model will be diveded in smaller parts and shared over several machine. \n",
        "\n",
        "For more Details watch Google Video [Distributed TensorFlow training ](https://www.youtube.com/watch?v=bRMGoPqsn20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTbdR3OrcNUw",
        "colab_type": "text"
      },
      "source": [
        "#E#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJi62gKrcT-D",
        "colab_type": "text"
      },
      "source": [
        "##Epoch##\n",
        "The term epoch describes one complete presentation of the data set. A large dataset has to be split in several small batches.\n",
        "\n",
        "As the number of epochs increases, more number of times the weight are changed in the neural network and the curve goes from underfitting to optimal to overfitting curve.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*i_lp_hUFyUD_Sq4pLer28g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIQxA74d0PeE",
        "colab_type": "text"
      },
      "source": [
        "#F#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNdGa53OKFnL",
        "colab_type": "text"
      },
      "source": [
        "#G#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS5bQMbQKihe",
        "colab_type": "text"
      },
      "source": [
        "##Gradient Desent##\n",
        "It is an iterative optimization algorithm used in machine learning to find the best results.\n",
        "Gradient means the rate of inclination or declination of a slope. Descent means the instance of descending.\n",
        "\n",
        "The algorithm is iterative means that we need to get the results multiple times to get the most optimal result. The iterative quality of the gradient descent helps a under-fitted graph to make the graph fit optimally to the data.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*pwPIG-GWHyaPVMVGG5OhAQ.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k0jJo2VZ_9t",
        "colab_type": "text"
      },
      "source": [
        "##Graphics processing uint (GPU)##\n",
        "GPUs are the devices when it comes to Deep Learning because a GPU has, compared to a CPU with 4 - 16 Cores, sometimes a thousend cores. So a GPU is able to more parallelize tasks and is therefore faster then CPUs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSDvgT_h0ShH",
        "colab_type": "text"
      },
      "source": [
        "#H#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOHpGpzjqAvF",
        "colab_type": "text"
      },
      "source": [
        "#I#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NomuayfdqEv7",
        "colab_type": "text"
      },
      "source": [
        "##Iterations##\n",
        "Iterations is the number of batches (see Batch) needed to complete one epoch (see Epoch). The number of batches is equal to number of iterations for one epoch.\n",
        "\n",
        "Let’s say we have 2000 training examples that we are going to use . We can divide the dataset of **2000** examples into **batches of 500** then it will take **4 iterations** to complete **1 epoch.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQhyFtdl0Ybh",
        "colab_type": "text"
      },
      "source": [
        "#K#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWwywo3o0Vly",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7VzzwGmLod2",
        "colab_type": "text"
      },
      "source": [
        "#L#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OcPnjFkL1Qb",
        "colab_type": "text"
      },
      "source": [
        "##Learning rate##\n",
        "The Gradient descent has a parameter called learning rate. As you can see below (left), initially the steps are bigger that means the learning rate is higher and as the point goes down the learning rate becomes more smaller by the shorter size of steps.\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*pwPIG-GWHyaPVMVGG5OhAQ.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH0Q8boA0fRv",
        "colab_type": "text"
      },
      "source": [
        "#M#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxvNShVk0imx",
        "colab_type": "text"
      },
      "source": [
        "#N#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-I4zcQ50ntv",
        "colab_type": "text"
      },
      "source": [
        "#O#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1y_nrB10rnQ",
        "colab_type": "text"
      },
      "source": [
        "##Overfitting##\n",
        "\n",
        "Overfitting is a term which is used, when a model is good in classifiying or predicting on a Dataset which was used for Training but not on a Dataset which it was not trained on.\n",
        "This is a common problem and comes mostly from a lack of sufficient and differsed datas. Means the more and differsed data you have the more the model fits. For example when your datasets contains just images of big dogs of less then 3 kinds, the model will not be able to recognize a smaller dog.\n",
        "\n",
        "Another way to prevent overfitting is to augmenting datas means to flip, rotate, zoom etc. images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqC-g9urLKq",
        "colab_type": "text"
      },
      "source": [
        "#P#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQwn2QT1rPhy",
        "colab_type": "text"
      },
      "source": [
        "##Perceptrons##\n",
        "Is a type of artificial neuron. Perceptrons were developed in the 1950s and 1960s by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. \n",
        "\n",
        "![alt text](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n",
        "\n",
        "The formula of the perceptrons is like this:\n",
        "\n",
        "$output =\n",
        "  \\begin{cases}\n",
        "   0       & \\quad \\text{if } w * x + b \\leq{ 0}\\\\\n",
        "   1  & \\quad \\text{if } w * x + b \\text{ < 0}\n",
        "  \\end{cases}$\n",
        "  \n",
        "As the perceptrons implements a NAND gate the output can be 0 or 1. Small changes to the weight to adjust the output have a big impact to the whole neural network.\n",
        "\n",
        "Because of that fact, Today, it's more common to use other models of artificial neurons like sigmoid, ReLU, TanH, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJll2MnnJiF",
        "colab_type": "text"
      },
      "source": [
        "#S#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygdPjMRnH_yg",
        "colab_type": "text"
      },
      "source": [
        "##Semi-Supervised Learning##\n",
        "\n",
        "Semi-supervised learning is a task that lies somewhere between supervised and unsupervised learning. You have a small amount of labelled samples, and you try to use the remaining unlabelled samples to get better performance at the learning task when compared to using only the labelled samples for supervised learning or all the samples in unsupervised learning..\n",
        "\n",
        "A special version of Semi-Supervised Learning is the ML Algorithm **Active Learning**\n",
        "\n",
        "[More information to this topic (PDF)](http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNd7fxacnPM6",
        "colab_type": "text"
      },
      "source": [
        "##Sigmoid neurons##\n",
        "The sigmoid neuron ist a perceptron using sigmoid as a activation function. In contrast to the simple NAND Gate used in the original perceptron this function, like a lot other similar functions, can pick a value between 0 and 1 (i.e. 0.666 etc) and make it possible to apply smaller changes to the Network.\n",
        "\n",
        "Sigmoid Notation:\n",
        "\n",
        "$f(x) = \\frac{1}{1 + exp- ^x}$\n",
        "\n",
        "Note: x is the $\\sum$ of $w_i \\cdot x_i + b$ means all weights (w) and inputs (x) of a neuron added with the bias (b)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iviTTKGa7grc",
        "colab_type": "text"
      },
      "source": [
        "##Supervised Learning##\n",
        "\n",
        "Supervised learning, in the context of artificial intelligence (AI) and machine learning, is a type of system in which both input and desired output data are provided. Input and output data are labelled for classification to provide a learning basis for future data processing. So all **datas** used in supervised learning **have to be labeled**.\n",
        "\n",
        "Another class of Machine Learning is **Semi-Supervised & Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPQDNHDJaHQ7",
        "colab_type": "text"
      },
      "source": [
        "#U#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBFFR7ylaMD_",
        "colab_type": "text"
      },
      "source": [
        "##Unsupervised Learning##\n",
        "Unsupervised learning is a method used to enable machines to classify both tangible and intangible objects without providing the machines any prior information about the objects. The things machines need to classify are varied, such as customer purchasing habits, behavioral patterns of bacteria and hacker attacks. The main idea behind unsupervised learning is to expose the machines to large volumes of varied unstructured and unlabeld data and allow it to learn and infer from the data. However, the machines must first be programmed to learn from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU1Xde40ZMSf",
        "colab_type": "text"
      },
      "source": [
        "#W#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8FpYYHBZRi-",
        "colab_type": "text"
      },
      "source": [
        "##Weights##\n",
        "Weights are values expressing the importance of the respective inputs to the output for a neuron. Together with the input value (x) and the bias (see Bias) it belogs to the transfer function of a neuron.  "
      ]
    }
  ]
}