{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Upper Confidence Bound",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakuronohana/my_datascience/blob/master/udemy/mlaz/Part%206%20-%20Reinforcement%20Learning/Section%2032%20-%20Upper%20Confidence%20Bound%20(UCB)/Python/upper_confidence_bound.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGjx38KQu4h7"
      },
      "source": [
        "# Reinforcement Learing\n",
        "\n",
        "Reinforcement Learning (RL) bildet neben Supervised und Unsupervised Learning die dritte große Gruppe von Machine Learning Verfahren. RL ist eine am natürlichen Lernverhalten des Menschen orientierte Methode. Menschliches Lernen erfolgt, insbesondere in frühen Stadien des Lernens, häufig über eine einfache Exploration der Umwelt. Dabei sind unsere Handlungen im Rahmen des Lernproblems durch einen gewissen Aktionsraum definiert. Über \"Trial and Error\" werden die Auswirkungen verschiedener Handlungen auf unsere Umwelt beobachtet und bewertet. Als Reaktion auf unsere Handlungen erhalten wir von unserer Umgebung ein Feedback, abstrakt dargestellt in Form einer Belohnung oder Bestrafung. Dabei ist das Konzept der Belohnung bzw. Bestrafung nur in den allerwenigsten Fällen monetär zu verstehen. In vielen Fällen wird die Belohnung in Form von sozialer Akzeptanz, Lob anderer Menschen aber auch durch persönliches Wohlbefinden oder Erfolgserlebnisse ausgezahlt. Vielfach zeigt sich auch eine zeitliche Latenz zwischen Handlung und Belohnung. Hierbei versucht der Mensch häufig, durch sein Handeln die erwartete \"Gesamtbelohnung\" im Zeitverlauf zu maximieren und nicht nur unmittelbare Belohnungen zu generieren.\n",
        "\n",
        "<img src='https://www.statworx.com/wp-content/uploads/reinforcement-learning.png' width='600'>\n",
        "\n",
        "Reinforcement Learning besteht formal betrachtet aus fünf wichtigen Komponenten, nämlich (1) dem Agenten (agent), (2) der Umgebung (environment), (3) dem Status (state), (4) der Aktion (action) sowie (5) der Belohnung (reward). Grundsätzlich lässt sich der Ablauf wie folgt beschreiben: Der Agent führt in einer Umgebung zu einem bestimmten Status ($s_t$) eine Aktion ($a_t$) aus dem zur Verfügung stehenden Aktionsraum A durch, die zu einer Reaktion der Umgebung in Form einer Belohnungen ($r_t$) führt.\n",
        "\n",
        "Die Reaktion der Umgebung auf die Aktion des Agenten beeinflusst nun wiederum die Wahl der Aktion des Agenten im nächsten Status ($s_{t+1}$). Über mehrere tausend, hunderttausend oder sogar millionen von Iterationen ist der Agent in der Lage, einen Zusammenhang zwischen seinen Aktionen und dem künftig zu erwartenden Nutzen in jedem Status zu approximieren und sich somit entsprechend optimal zu verhalten. Dabei befindet sich der Agent immer in einem Dilemma zwischen der Nutzung seiner bisher erworbenen Erfahrung auf der einen und der Exploration neuer Strategien zur Erhöhung der Belohnung auf der anderen Seite. Dies wird als \"Exploration-Exploitation Dilemma\" bezeichnet.\n",
        "\n",
        "Die Approximation des Nutzens kann dabei modellfrei, also über reine Exploration der Umgebung erfolgen oder durch die Anwendung von Machine Learning Modellen, die den Nutzen einer Aktion versuchen zu approximieren. Letztere Variante wird insbesondere dann angewendet, wenn der Status- und/oder Aktionsraum von hoher Dimensionalität ist\n",
        "\n",
        "Quelle: [STATWORX Blog](https://www.statworx.com/ch/blog/einfuehrung-in-reinforcement-learning-wenn-maschinen-wie-menschen-lernen/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKzwx56Yfmzn"
      },
      "source": [
        "## Multi-Armed Bandit Problem\n",
        "\n",
        "Das Dilemma zwischen Exploration (Erkundung) und Exploitation (Ausbeutung) besteht in vielen Aspekten unseres Lebens. Sagen wir, Ihr Lieblingsrestaurant ist gleich um die Ecke. Wenn Sie jeden Tag dorthin gehen, wären Sie zuversichtlich, was Sie bekommen werden, aber Sie verpassen die Chance, eine noch bessere Option zu entdecken. Wenn Sie ständig neue Orte ausprobieren, werden Sie sehr wahrscheinlich von Zeit zu Zeit unangenehmes Essen zu sich nehmen müssen. In ähnlicher Weise versuchen Online-Berater, ein Gleichgewicht zwischen den bekanntermaßen attraktivsten Anzeigen und den neuen Anzeigen zu finden, die vielleicht sogar noch erfolgreicher sind.\n",
        "\n",
        "Wenn wir alle Informationen über die Umwelt gelernt haben, sind wir in der Lage, die beste Strategie zu finden, indem wir auch nur Brute-Force simulieren, ganz zu schweigen von vielen anderen intelligenten Ansätzen. Das Dilemma ergibt sich aus den unvollständigen Informationen: Wir müssen genügend Informationen sammeln, um die besten Gesamtentscheidungen zu treffen und gleichzeitig das Risiko unter Kontrolle zu halten. Mit Ausbeutung nutzen wir die beste Option, die wir kennen. Bei der Exploration gehen wir ein gewisses Risiko ein, um Informationen über unbekannte Optionen zu sammeln. Die beste langfristige Strategie kann mit kurzfristigen Opfern verbunden sein. Zum Beispiel könnte ein Explorationsversuch ein totaler Fehlschlag sein, aber das warnt uns davor, diese Maßnahme in Zukunft nicht zu oft zu ergreifen.\n",
        "\n",
        "Das Problem der **mehrarmigen Banditen (Multi-Armed Bandit)** ist ein klassisches Problem, das das Dilemma Exploration vs. Ausbeutung deutlich macht. Stellen Sie sich vor, Sie befinden sich in einem Casino und stehen mehreren Spielautomaten gegenüber, von denen jeder mit einer unbekannten Wahrscheinlichkeit konfiguriert ist, wie wahrscheinlich es ist, dass Sie bei einem Spiel eine Belohnung erhalten können. Die Frage ist: Was ist die beste Strategie, um langfristig die höchsten Belohnungen zu erzielen?\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/22970879/41629289-ec662a58-73e5-11e8-9f41-40c6d7ba5a36.jpg'>\n",
        "\n",
        "\n",
        "Die Beschränkung auf eine endliche Anzahl von Versuchen führt zu einer neue Art von Explorationsproblem. Wenn z.B. die Anzahl der Versuche kleiner ist als die Anzahl der Spielautomaten, können wir nicht einmal jeden Automaten ausprobieren, um die Belohnungswahrscheinlichkeit (!) abzuschätzen, und müssen uns daher mit einer begrenzten Menge an Wissen und Ressourcen (d.h. Zeit) klug verhalten.\n",
        "\n",
        "Ein naiver Ansatz kann darin bestehen, dass man mit einem Automaten sehr viele Runden lang weiterspielt, um schließlich die \"wahre\" Belohnungswahrscheinlichkeit nach dem Gesetz der großen Zahlen zu schätzen. Dies ist jedoch ziemlich verschwenderisch und garantiert sicherlich nicht die beste langfristige Belohnung.\n",
        "\n",
        "Je nachdem, wie wir Exploration betreiben, gibt es mehrere Möglichkeiten, das Problem der mehrarmigen Banditen zu lösen.\n",
        "\n",
        "* Keine Erkundung: der naivste Ansatz und ein schlechter.\n",
        "* Exploration nach dem Zufallsprinzip\n",
        "* Exploration klug mit Vorzug vor Unsicherheit\n",
        "\n",
        "\n",
        "Quelle: [Lil Log](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJdrJJQUv3-k"
      },
      "source": [
        "## Upper Confidence Bound (UCB)\n",
        "\n",
        "Ein Weg um das Multi-Armed-Bandit Problem zu lösen ist der UCB-Algorithmus.\n",
        "Der UCB-Algorithmus (Upper Confidence Bound) wird oft als \"Optimismus angesichts der Unsicherheit\" formuliert. Um zu verstehen, warum, bedenken Sie in einer bestimmten Runde, dass die Belohnungsfunktion jedes Arms als eine Punktschätzung auf der Grundlage der beobachteten durchschnittlichen Belohnungsrate wahrgenommen werden kann. Indem wir die Intuition aus [Konfidenzintervallen](https://www.statistik-nachhilfe.de/ratgeber/statistik/induktive-statistik/konfidenzintervall-fuer-erwartungswert-varianz-und-median) ableiten, können wir für jede Punktschätzung auch eine Form der Unsicherheitsgrenze um die Punktschätzung herum einbeziehen. In diesem Sinne haben wir sowohl eine untere Grenze als auch eine obere Grenze für jeden Arm.\n",
        "Der UCB-Algorithmus hat einen treffenden Namen, weil wir uns nur mit der oberen Grenze befassen, da wir versuchen, den Arm mit der höchsten Belohnungsrate zu finden.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XCjepjJwEv-"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mBkG3YwNTt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npqlXjaNwYTv"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMJfUVLVwcFc"
      },
      "source": [
        "datloc = 'https://raw.githubusercontent.com/sakuronohana/my_datascience/master/udemy/mlaz/Part%206%20-%20Reinforcement%20Learning/Section%2032%20-%20Upper%20Confidence%20Bound%20(UCB)/Python/Ads_CTR_Optimisation.csv'\n",
        "dataset = pd.read_csv(datloc)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66I-mZM1ryF5",
        "outputId": "79d8d376-50da-4bb7-ddd5-0fb8e5a3f096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaSbots_wfoB"
      },
      "source": [
        "### Implementing UCB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXftWcjDwsYj"
      },
      "source": [
        "### Visualising the results"
      ]
    }
  ]
}