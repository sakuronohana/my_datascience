{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Upper Confidence Bound",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakuronohana/my_datascience/blob/master/udemy/mlaz/Part%206%20-%20Reinforcement%20Learning/Section%2032%20-%20Upper%20Confidence%20Bound%20(UCB)/Python/upper_confidence_bound.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGjx38KQu4h7"
      },
      "source": [
        "# Reinforcement Learing\n",
        "\n",
        "Reinforcement Learning (RL) bildet neben Supervised und Unsupervised Learning die dritte große Gruppe von Machine Learning Verfahren. RL ist eine am natürlichen Lernverhalten des Menschen orientierte Methode. Menschliches Lernen erfolgt, insbesondere in frühen Stadien des Lernens, häufig über eine einfache Exploration der Umwelt. Dabei sind unsere Handlungen im Rahmen des Lernproblems durch einen gewissen Aktionsraum definiert. Über \"Trial and Error\" werden die Auswirkungen verschiedener Handlungen auf unsere Umwelt beobachtet und bewertet. Als Reaktion auf unsere Handlungen erhalten wir von unserer Umgebung ein Feedback, abstrakt dargestellt in Form einer Belohnung oder Bestrafung. Dabei ist das Konzept der Belohnung bzw. Bestrafung nur in den allerwenigsten Fällen monetär zu verstehen. In vielen Fällen wird die Belohnung in Form von sozialer Akzeptanz, Lob anderer Menschen aber auch durch persönliches Wohlbefinden oder Erfolgserlebnisse ausgezahlt. Vielfach zeigt sich auch eine zeitliche Latenz zwischen Handlung und Belohnung. Hierbei versucht der Mensch häufig, durch sein Handeln die erwartete \"Gesamtbelohnung\" im Zeitverlauf zu maximieren und nicht nur unmittelbare Belohnungen zu generieren.\n",
        "\n",
        "<img src='https://www.statworx.com/wp-content/uploads/reinforcement-learning.png' width='600'>\n",
        "\n",
        "Reinforcement Learning besteht formal betrachtet aus fünf wichtigen Komponenten, nämlich (1) dem Agenten (agent), (2) der Umgebung (environment), (3) dem Status (state), (4) der Aktion (action) sowie (5) der Belohnung (reward). Grundsätzlich lässt sich der Ablauf wie folgt beschreiben: Der Agent führt in einer Umgebung zu einem bestimmten Status ($s_t$) eine Aktion ($a_t$) aus dem zur Verfügung stehenden Aktionsraum A durch, die zu einer Reaktion der Umgebung in Form einer Belohnungen ($r_t$) führt.\n",
        "\n",
        "Die Reaktion der Umgebung auf die Aktion des Agenten beeinflusst nun wiederum die Wahl der Aktion des Agenten im nächsten Status ($s_{t+1}$). Über mehrere tausend, hunderttausend oder sogar millionen von Iterationen ist der Agent in der Lage, einen Zusammenhang zwischen seinen Aktionen und dem künftig zu erwartenden Nutzen in jedem Status zu approximieren und sich somit entsprechend optimal zu verhalten. Dabei befindet sich der Agent immer in einem Dilemma zwischen der Nutzung seiner bisher erworbenen Erfahrung auf der einen und der Exploration neuer Strategien zur Erhöhung der Belohnung auf der anderen Seite. Dies wird als \"Exploration-Exploitation Dilemma\" bezeichnet.\n",
        "\n",
        "Die Approximation des Nutzens kann dabei modellfrei, also über reine Exploration der Umgebung erfolgen oder durch die Anwendung von Machine Learning Modellen, die den Nutzen einer Aktion versuchen zu approximieren. Letztere Variante wird insbesondere dann angewendet, wenn der Status- und/oder Aktionsraum von hoher Dimensionalität ist\n",
        "\n",
        "Quelle: [STATWORX Blog](https://www.statworx.com/ch/blog/einfuehrung-in-reinforcement-learning-wenn-maschinen-wie-menschen-lernen/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKzwx56Yfmzn"
      },
      "source": [
        "## Multi-Armed Bandit Problem\n",
        "\n",
        "Das Dilemma zwischen Exploration (Erkundung) und Exploitation (Ausbeutung) besteht in vielen Aspekten unseres Lebens. Sagen wir, Ihr Lieblingsrestaurant ist gleich um die Ecke. Wenn Sie jeden Tag dorthin gehen, wären Sie zuversichtlich, was Sie bekommen werden, aber Sie verpassen die Chance, eine noch bessere Option zu entdecken. Wenn Sie ständig neue Orte ausprobieren, werden Sie sehr wahrscheinlich von Zeit zu Zeit unangenehmes Essen zu sich nehmen müssen. In ähnlicher Weise versuchen Online-Berater, ein Gleichgewicht zwischen den bekanntermaßen attraktivsten Anzeigen und den neuen Anzeigen zu finden, die vielleicht sogar noch erfolgreicher sind.\n",
        "\n",
        "Wenn wir alle Informationen über die Umwelt gelernt haben, sind wir in der Lage, die beste Strategie zu finden, indem wir auch nur Brute-Force simulieren, ganz zu schweigen von vielen anderen intelligenten Ansätzen. Das Dilemma ergibt sich aus den unvollständigen Informationen: Wir müssen genügend Informationen sammeln, um die besten Gesamtentscheidungen zu treffen und gleichzeitig das Risiko unter Kontrolle zu halten. Mit Ausbeutung nutzen wir die beste Option, die wir kennen. Bei der Exploration gehen wir ein gewisses Risiko ein, um Informationen über unbekannte Optionen zu sammeln. Die beste langfristige Strategie kann mit kurzfristigen Opfern verbunden sein. Zum Beispiel könnte ein Explorationsversuch ein totaler Fehlschlag sein, aber das warnt uns davor, diese Maßnahme in Zukunft nicht zu oft zu ergreifen.\n",
        "\n",
        "Das Problem der **mehrarmigen Banditen (Multi-Armed Bandit)** ist ein klassisches Problem, das das Dilemma Exploration vs. Ausbeutung deutlich macht. Stellen Sie sich vor, Sie befinden sich in einem Casino und stehen mehreren Spielautomaten gegenüber, von denen jeder mit einer unbekannten Wahrscheinlichkeit konfiguriert ist, wie wahrscheinlich es ist, dass Sie bei einem Spiel eine Belohnung erhalten können. Die Frage ist: Was ist die beste Strategie, um langfristig die höchsten Belohnungen zu erzielen?\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/22970879/41629289-ec662a58-73e5-11e8-9f41-40c6d7ba5a36.jpg'>\n",
        "\n",
        "\n",
        "Die Beschränkung auf eine endliche Anzahl von Versuchen führt zu einer neue Art von Explorationsproblem. Wenn z.B. die Anzahl der Versuche kleiner ist als die Anzahl der Spielautomaten, können wir nicht einmal jeden Automaten ausprobieren, um die Belohnungswahrscheinlichkeit (!) abzuschätzen, und müssen uns daher mit einer begrenzten Menge an Wissen und Ressourcen (d.h. Zeit) klug verhalten.\n",
        "\n",
        "Ein naiver Ansatz kann darin bestehen, dass man mit einem Automaten sehr viele Runden lang weiterspielt, um schließlich die \"wahre\" Belohnungswahrscheinlichkeit nach dem Gesetz der großen Zahlen zu schätzen. Dies ist jedoch ziemlich verschwenderisch und garantiert sicherlich nicht die beste langfristige Belohnung.\n",
        "\n",
        "Je nachdem, wie wir Exploration betreiben, gibt es mehrere Möglichkeiten, das Problem der mehrarmigen Banditen zu lösen.\n",
        "\n",
        "* Keine Erkundung: der naivste Ansatz und ein schlechter.\n",
        "* Exploration nach dem Zufallsprinzip\n",
        "* Exploration klug mit Vorzug vor Unsicherheit\n",
        "\n",
        "\n",
        "Quelle: [Lil Log](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJdrJJQUv3-k"
      },
      "source": [
        "## Upper Confidence Bound (UCB)\n",
        "\n",
        "Ein Weg um das Multi-Armed-Bandit Problem zu lösen ist der UCB-Algorithmus.\n",
        "Der UCB-Algorithmus (Upper Confidence Bound) wird oft als \"Optimismus angesichts der Unsicherheit\" formuliert. Um zu verstehen, warum, bedenken Sie in einer bestimmten Runde, dass die Belohnungsfunktion jedes Arms als eine Punktschätzung auf der Grundlage der beobachteten durchschnittlichen Belohnungsrate wahrgenommen werden kann. Indem wir die Intuition aus [Konfidenzintervallen](https://www.statistik-nachhilfe.de/ratgeber/statistik/induktive-statistik/konfidenzintervall-fuer-erwartungswert-varianz-und-median) ableiten, können wir für jede Punktschätzung auch eine Form der Unsicherheitsgrenze um die Punktschätzung herum einbeziehen. In diesem Sinne haben wir sowohl eine untere Grenze als auch eine obere Grenze für jeden Arm.\n",
        "Der UCB-Algorithmus hat einen treffenden Namen, weil wir uns nur mit der oberen Grenze befassen, da wir versuchen, den Arm mit der höchsten Belohnungsrate zu finden.\n",
        "\n",
        "Der UCB-Algorithmus gehört zu den am meist verwendesten RL-Algorithmen im Zusammenhang mit Online Advertisements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XCjepjJwEv-"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mBkG3YwNTt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npqlXjaNwYTv"
      },
      "source": [
        "### Importing the dataset\n",
        "\n",
        "Dieser Datensatz bernötigen wir zu Simulation einer Webseite, auf welcher 10 verschiedene Werbungen eines Autos aufgeschaltet werden. Die Webseite wird von 10000 Benutzern besucht. Jeder der Benutzer klickt auf die Ads welche ihn am meisten ansprechen. In einem realen Business-Case würde der Reenforcement Learning Algorithmus mittels realtime Daten mit minimalen Aufwand lernen, welche Ads am meisten angeklickt bzw. am meisten gezeigt werden sollten. Wichtig ist es dabei zu wissen, dass jedes Advertisment welches aufgeschaltet wird je nach Plazierung viel Geld kostet. Das Marketing des Autoherstellers möchte somit so schnell wie möglich den am \"gewinnbringensten\" Ad ermitteln."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMJfUVLVwcFc"
      },
      "source": [
        "datloc = 'https://raw.githubusercontent.com/sakuronohana/my_datascience/master/udemy/mlaz/Part%206%20-%20Reinforcement%20Learning/Section%2032%20-%20Upper%20Confidence%20Bound%20(UCB)/Python/Ads_CTR_Optimisation.csv'\n",
        "dataset = pd.read_csv(datloc)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66I-mZM1ryF5",
        "outputId": "b8bec596-7795-4265-a1ed-3cc94d718e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "print(dataset.shape)\n",
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ad 1</th>\n",
              "      <th>Ad 2</th>\n",
              "      <th>Ad 3</th>\n",
              "      <th>Ad 4</th>\n",
              "      <th>Ad 5</th>\n",
              "      <th>Ad 6</th>\n",
              "      <th>Ad 7</th>\n",
              "      <th>Ad 8</th>\n",
              "      <th>Ad 9</th>\n",
              "      <th>Ad 10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Ad 1  Ad 2  Ad 3  Ad 4  Ad 5  Ad 6  Ad 7  Ad 8  Ad 9  Ad 10\n",
              "0     1     0     0     0     1     0     0     0     1      0\n",
              "1     0     0     0     0     0     0     0     0     1      0\n",
              "2     0     0     0     0     0     0     0     0     0      0\n",
              "3     0     1     0     0     0     0     0     1     0      0\n",
              "4     0     0     0     0     0     0     0     0     0      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaSbots_wfoB"
      },
      "source": [
        "### Implementing UCB\n",
        "\n",
        "Anschliessend werden wir die folgende Schritte mittels Python implementieren:\n",
        "\n",
        "<img src='https://github.com/sakuronohana/my_datascience/blob/master/udemy/mlaz/Part%206%20-%20Reinforcement%20Learning/Section%2032%20-%20Upper%20Confidence%20Bound%20(UCB)/UCB-Steps.jpg?raw=true' width='600'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnvZKJ5UqIFA"
      },
      "source": [
        "import math\n",
        "# Step 1 \n",
        "N = 10000 # Anzahl Runden bzw. Benutzer welche auf Ads klicken\n",
        "d = 10 # Anzahl Ads\n",
        "ads_selected = [] # Leere Liste für die angeklickten Ads\n",
        "numbers_of_selections = [0] * d # Leere Liste für die Anzahl der gewählten Ads\n",
        "sums_of_rewards = [0] * d # Liste für die Summe aller Belohnungen\n",
        "total_reward = 0 # \n",
        "\n",
        "# Step 2 \n",
        "for n in range(0,N): # 1. Iteration für die Runden (0 bis 10000)\n",
        "  ad = 0 # Started jede Iteration mit Ad 1\n",
        "  max_upper_bound = 0 # Started am Anfang mit der obersten Vertrauensgrenze von 0 welche jedes Runde aktualisiert wird.\n",
        "\n",
        "  for i in range(0,d): # 2. Iteration für Ads (0 bis 9)\n",
        "    if (numbers_of_selections[i] > 0): # Falls ausgewählte Ad ist grösser als 0\n",
        "      # Computing average reward\n",
        "      average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n",
        "      # Computing confidence interval\n",
        "      delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])\n",
        "      upper_bound = average_reward + delta_i\n",
        "   \n",
        "    else:\n",
        "      # Step 3 \n",
        "      upper_bound = 1e400 # Trick um eine Zahl fast auf endlos zu setzen\n",
        "    if (upper_bound > max_upper_bound):\n",
        "      max_upper_bound = upper_bound\n",
        "      ad = i\n",
        "\n",
        "  ads_selected.append(ad)\n",
        "  numbers_of_selections[ad] = numbers_of_selections[ad] + 1 # += 1 ist das gleiche wie numbers_of_selections[ad] + 1\n",
        "  reward = dataset.values[n, ad]\n",
        "  sums_of_rewards[ad] = sums_of_rewards[ad] + reward\n",
        "  total_reward = total_reward + reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozQRDby7BRh1",
        "outputId": "72561bd0-75b5-454f-b43e-eee1ad402208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "(3/2 * math.log(n + 1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.815510557964275"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXftWcjDwsYj"
      },
      "source": [
        "### Visualising the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uV3N9Mo9PGE",
        "outputId": "32ab16a0-9a4e-4ac8-a314-5b8bceb5f257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.hist(ads_selected)\n",
        "plt.title('Histogramm of ads selections')\n",
        "plt.xlabel('Ads')\n",
        "plt.ylabel('Number of times each ad was selected')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p92KenfABSR4"
      },
      "source": [
        "Wir können anhand des Histogramms erkennen, dass der UCB-Algoritmus einen klaren Favoriten erkoren hat. Ads Nummer 4 scheint mit Abstand am beliebtesten zu sein. Was wir hier auch klar erkennen können ist, dass UCB auch mit weniger Runden auskommen würde, was natürlich unser Ziel wäre.\n",
        "Wie es scheint könnten wir es wagen, die Rudenzahl auf 1000 zu reduzieren, was zu folgendem Resultat geführt hat:\n",
        "\n",
        "<img src='https://github.com/sakuronohana/my_datascience/blob/master/udemy/mlaz/Part%206%20-%20Reinforcement%20Learning/Section%2032%20-%20Upper%20Confidence%20Bound%20(UCB)/UCB_Hist_Result.jpg?raw=true' width='400'>\n",
        "\n",
        "Wir könnten jetzt sicher noch etwas optimieren, jedoch spätenstens bei einer Rundenanzahl von 500 ist Schluss:\n",
        "\n",
        "<img src='https://github.com/sakuronohana/my_datascience/blob/master/udemy/mlaz/Part%206%20-%20Reinforcement%20Learning/Section%2032%20-%20Upper%20Confidence%20Bound%20(UCB)/UCB_Hist_Result_2.png?raw=true' width='400'>"
      ]
    }
  ]
}