{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj",
        "colab_type": "text"
      },
      "source": [
        "## Importing the libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cfa1xi08LGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT",
        "colab_type": "text"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udp0ENgV8YXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "daturl = 'https://raw.githubusercontent.com/sakuronohana/my_datascience/master/udemy/mlaz/Part%201%20-%20Data%20Preprocessing/Python/Data.csv'\n",
        "\n",
        "dataset = pd.read_csv(daturl)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr-QORvy89Q7",
        "colab_type": "text"
      },
      "source": [
        "### Selecting data\n",
        "\n",
        "Wenn wir Supervised Learning durchführen, dann besteht der Datensatz meist aus einem **Label** (y, abhängigen Variable / dependent Variable) und den **Merkmalen** (Features, X, unabhängigen Variablen / independent Variables).\n",
        "Es ist Standard, das in einem Datensatz das Label meist in der hintersten Spalte ist.\n",
        "\n",
        "Wir importieren nun die die Labels und Features in verschieden Matrixen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2M044_d-3Ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = dataset.iloc[:,:-1].values # Erste Doppelpunkt ist der Range mit Rows und zweiter Doppelpunkt der Range mit Columns. Mit Values erstellen wir eine Matrix mit den Werten ohne Header\n",
        "y = dataset.iloc[:,-1].values"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk75msSU_578",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e23b5e95-0607-41b9-e757-8d6e99f9408c"
      },
      "source": [
        "print('Hier sind die Labels:',y)\n",
        "print('Hier sind die Features:\\n',X)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hier sind die Labels: ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n",
            "Hier sind die Features:\n",
            " [['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC",
        "colab_type": "text"
      },
      "source": [
        "## Taking care of missing data\n",
        "\n",
        "Es kommt des öfteren vor, dass in einem Datensatz Werte fehlen. Die Art und Weise wie man mit fehlenden Daten umgeht ist unterschiedlich und geht von den Werten ersetzen (mit Mean, Median usw.) bis löschen, was bei einem grossen Datensatz meist kein Problem darstellt jedoch bei einem kleinen schon.\n",
        "\n",
        "Im folgenden Beispiel werden wir die fehlenden Werte mittels arithm. Mittelwert ersetzen. Dazu verwenden wir eines der beliebtesten ML Tools **scikit-learn**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2MLiqLRCGA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "f15e65d0-7d85-4683-c4a7-c8178baedbeb"
      },
      "source": [
        "# Schauen wir mal zuerst wo wir fehlende Wert haben\n",
        "dataset.isna()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Age</th>\n",
              "      <th>Salary</th>\n",
              "      <th>Purchased</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Country    Age  Salary  Purchased\n",
              "0    False  False   False      False\n",
              "1    False  False   False      False\n",
              "2    False  False   False      False\n",
              "3    False  False   False      False\n",
              "4    False  False    True      False\n",
              "5    False  False   False      False\n",
              "6    False   True   False      False\n",
              "7    False  False   False      False\n",
              "8    False  False   False      False\n",
              "9    False  False   False      False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVb4RJ42C6bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Um alle fehlenden Werte in den Features zu ersetzen verwenden wir SimpleImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Zuerst ersellen eine Instanz der Klasse SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean') # Der erste Wert in der Klammer bedeutet, was wir ersetzen und der zweite Wert mit was\n",
        "\n",
        "# Nun benötigen wir noch eine Funktion mit welcher wir die Klasse auf unseren Datensatz anwenden können. Hierzu steht die Funktion fit\n",
        "imputer.fit(X[:,1:3]) #im Datensatz werden wir in allen nummerischen Features nach fehlendne Werten suchen\n",
        "\n",
        "# Nachfolgend führe wir nun die Transformation des Datensatzes durch\n",
        "X[:,1:3] = imputer.transform(X[:,1:3]) #Die Änderungen werden direkt im Datensatz durchgeführt"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5HAiLbYi5XS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "07d072d2-1630-48df-fc46-e760ca7a2e7a"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK",
        "colab_type": "text"
      },
      "source": [
        "## Encoding categorical data\n",
        "\n",
        "Ein Machine Learning Algorithmus kann mit kategorische Werten nicht umgehen, weshalb diese in einem speziellen Verfahren, dass sich Encoding nennt, in nummerische Werte umgewandelt werden müssen.\n",
        "\n",
        "Das OneHot-Encoding wandelt alle gleiche Features in einen spezifischen Vektor um.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh",
        "colab_type": "text"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_4poE7X9KAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wir Laden nun das entsprechende Modul von Scikit-learn\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Nun erstellen wir eine Klasse und speichern diese direkt in einer Funktion\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])], remainder='passthrough') # Erstes Argument ist der Encoder mit der Spalte, zweites Argument ignoriert die restlichen Spalten\n",
        "X = np.array(ct.fit_transform(X)) # ML Algorithmen erwarten grundsätzlich immer eine Array weshalb wir die neuen Spalten auch entsprechendn konvertieren müssen"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjHv9MN7gLsX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "36d90a61-4c1f-4f07-ce49-74d08c597e1f"
      },
      "source": [
        "print('Unser kategorischen Fearture Country besteht nun aus 3 Spalten und wurde transformiert in ein für die Machine lesbares Format \\n', X)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unser kategorischen Fearture Country besteht nun aus 3 Spalten und wurde transformiert in ein für die Machine lesbares Format \n",
            " [[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6",
        "colab_type": "text"
      },
      "source": [
        "### Encoding the Dependent Variable\n",
        "\n",
        "Nun Kodieren wir noch den Label-Datensatz. Dieses Encoding ist leicht anders als das von X, da wir beim Label nur zwei Zustände (Yes/No) haben. Weshalb wir hier mit dem sogenannten Label Encoding arbeiten können.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtd3fhDmjgeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder() # Erstellen einer Klasse\n",
        "y = le.fit_transform(y) # Da beim LabelEncoding keine zusätzlichen Spalten hinzugefügt werden muss hier kein Array erstellt werden."
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy-c4CILkWIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a1e610d-da74-42b3-d4c6-a25856f9f595"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set\n",
        "\n",
        "Nun müssen wir noch die Datensätze X und y in einen Trainings- und Testdatensatz aufteilen. Hierzu hat Scikit Learn ebensfalls ein entsprechendens Tool zur Hand. Die allerseits gängige Grösse des Splits ist 80/20 d.h. 80% Trainingsdaten 20% Testdaten.\n",
        "\n",
        "An dieser Stelle möchten wir noch eine wichtige Frage klären. Teilen wir einen Datensatz vor oder nach dem Feature Scaling in jeweils ein Trainings- und Testset auf? Die Antwort ist ganz einfach. **Vor dem Feature Scaling**\n",
        "\n",
        "Der Grund dafür ist eben so einfach wie die Antwort. Wir wollen, dass die Testdaten nicht in der gleichen Art skaliert werden wie die Trainingsdaten. Würde wir den umgekehrten Weg beschreiten, dann könntes es zu einem sogenannten [Data Leakage](https://machinelearningmastery.com/data-leakage-machine-learning/) führen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQmRKolLlqxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "'''\n",
        "Nachfolgend teilen wir die Datensätze in Traing und Test auf, dabei definieren wir alle Variablen X_train usw.\n",
        "geben in den Argumenten mit, welche Datensatz wir spliten, wie gross der Testset sein wird und mit welchem Random Seed\n",
        "wir arbeiten.\n",
        "'''\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 1 )"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_kWRhMjq9nb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "370c2468-4896-48f6-f4ca-f52951ed1b88"
      },
      "source": [
        "print(X_train, X_test)\n",
        "print(y_train,y_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]] [[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n",
            "[0 1 0 0 1 1 0 1] [0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR",
        "colab_type": "text"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Nun kommen wir zu einem anderen wichtigen Thema der Datenvorbereitung - Feature Scaling.\n",
        "\n",
        "Ziel der Skalierung ist es zu verhindern, dass während der ML Trainingsphase die einen Features nicht zu stark von den anderen Features dominiert bzw. ignoriert werden. Hierbei ist noch zu erwähnen, dass nicht bei allen ML Methoden eine Skalierung der Daten notwendig ist.\n",
        "\n",
        "Es gibt zwei hauptsächliche Feature Scaling Methoden:\n",
        "\n",
        "* Standardisierung - Transformiert die Daten in Werte -1 + 1. Diese Methode kann in den meisten der Datensätze angewendet werden. Somit gehört sie zu den beliebtesten Methoden.\n",
        "* Normalisierung - Transformiert die Daten in Werte von 0 - 1. Diese Methode ist nur dann zulässig, wenn eine Normalverteilung im Datensatz besteht.\n",
        "\n",
        "In diesem Zusammenhang gibt es noch eine wichtige Frage zu klären. \n",
        "**Muss man Feature Scaling auch auf die kodierten kategorischen Features anwenden?** die Antwort auch hier ist ganz einfach - **Nein**. Diese kodierten kategorischen Features sind ja bereits in Dummy-Werten von 0 und 1 umgewandelt worden. Würde wir diese noch einmal umwandeln, dann bekommen wir jedesmal komische Werte, mittels denen wir ja die ursprünglichen Zuordnung nicht mehr machen könnten.\n",
        "\n",
        "### Bedeutung von fit und transform\n",
        "\n",
        "Im Zusammenhang mit der Standardskalierung von Trainings- und Testdaten ist es wichtig den Unterschied zwischen der Funktion fit und transform zu wissen:\n",
        "\n",
        "\"fit\" berechnet den Mittelwert und den Standard, die für die spätere Skalierung verwendet werden sollen. (nur eine Berechnung), dir wird nichts gegeben .\n",
        "\n",
        "\"transform\" verwendet einen zuvor berechneten Mittelwert und Standard, um die Daten automatisch zu skalieren (Mittelwert von allen Werten subtrahieren und dann durch Standard dividieren).\n",
        "\n",
        "\"fit_transform\" macht beides gleichzeitig. Sie können dies also mit 1 Codezeile anstelle von 2 tun.\n",
        "\n",
        "Schauen wir uns das jetzt in der Praxis an:\n",
        "\n",
        "Für den X-Trainingssatz führen wir \"fit_transform\" aus, da wir Mittelwert und Standard berechnen und dann zur automatischen Skalierung der Daten verwenden müssen. Für den X- Testsatz haben wir bereits den Mittelwert und den Standard, also machen wir nur den Teil \"transformieren\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I33aV5zuuK2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler() # Wir erstellen wie gewohnt eine Klasse\n",
        "\n",
        "\n",
        "X_train[:,3:] = sc.fit_transform(X_train[:,3:]) #fit holt die erforderlichen Werte wie Mean und SD, Transform macht die Standardisierung\n",
        "\n",
        "X_test[:,3:] = sc.Transform(X_test[:,3:]) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}