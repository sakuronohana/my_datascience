{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hierarchical Clustering",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakuronohana/my_datascience/blob/master/udemy/mlaz/Part%204%20-%20Clustering/Section%2025%20-%20Hierarchical%20Clustering/Python/hierarchical_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKkbeQi2Mzug"
      },
      "source": [
        "# Hierarchical Clustering (HC)\n",
        "\n",
        "Wie auch schon bei k-Means geht es bei HC ebenfalls darum abhänige Variablen mittels Bildung von Clustern zu finden. Der hauptsächliche Unterschied zwischen den beiden Algorithmen definiert sich über die Art und Weise wie die Bildung der Cluster erfolgt. Während bei k-Means sogenannte Centroids in Mitten die Datenpunkt gesetzt werden, werden bei HC die nach und nach die nächstgelegenen Datenpunkte zu Clustern zusammengeführt. Es gibt hauptsächlich zwei Arten von hierarchischem Clustering. Eines ist Agglomerative Hierarchical Clustering und das andere ist Divisive Hierarchical Clustering.\n",
        "\n",
        "**Agglomeratives hierarchisches Clustering** beginnt damit, dass jede Beobachtung als separates Cluster behandelt wird. Anschließend werden wiederholt die folgenden zwei Schritte ausgeführt: (1) Identifizieren der beiden Cluster, die am nächsten beieinander liegen, und (2) Zusammenführen der beiden ähnlichsten Cluster. Dies wird fortgesetzt, bis alle Cluster zusammengeführt sind:\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*bQvm8on_47ifJGyOowGCow.png' width='600'>\n",
        "\n",
        "Theoretisch kann dies auch erreicht werden, indem zunächst alle Beobachtungen in einem Cluster zusammengefasst und diese Cluster dann aufgeteilt werden. Dieser ist als **divisives hierarchisches Clustering** bekannt.\n",
        "\n",
        "Die folgenden Abbildungen zeigen, wie der euklidische Abstand zweier Punkte und einige Ansätze für den Abstand zwischen zwei Clustern berechnet werden.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*lUSItZxfdAMK3TfZh7Mceg.png' width='800'>\n",
        "\n",
        "Der agglomerative hierarchische Clustering-Algorithmus funktioniert wie in der folgenden Abbildung dargestellt. Es beginnt mit den nächsten zwei Punkten und verringern nacheinander die Anzahl der Cluster auf einen.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*VcKlANXX-zivNaucjNU82w.png' width='700'>\n",
        "\n",
        "Um die tatsächlichen Clusternummern zu bestimmen wird nun ein Dendrogramm erstellt. Ein Dendrogramm ist ein baumartiges Diagramm, das die Folgen von Zusammenführungen oder Teilungen aufzeichnet. Wir können schnell erkennen, wie viele Cluster wir an einem bestimmten Schwellenwert haben werden, indem wir uns nur ansehen, wie viele vertikale Linien dieser horizontale Schwellenwert tatsächlich überschreitet.\n",
        "In diesem Fall können wir feststellen, dass die beiden Cluster am besten zu teilen sind. Der Trick besteht darin, die Linien zu kreuzen, die die Linie mit dem größten Abstand enthalten.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*tGQJO20wTOFen5E_wkDiAA.png' width='700'>\n",
        "\n",
        "Hierarchisches Clustering kann mit Big Data **nicht** gut umgehen, k-Means Clustering jedoch schon. Dies liegt daran, dass die zeitliche Komplexität bei k-Means linear ist, während die der hierarchischen Clusterbildung quadratisch ist. Daher ist k-Means die beliebtere Clustering-Methode. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaQI437hM1Ho"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2-yCxz3Tmtm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFeTEtDxM7K4"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YczqU2l4T3NN"
      },
      "source": [
        "datloc = 'https://raw.githubusercontent.com/sakuronohana/my_datascience/master/udemy/mlaz/Part%204%20-%20Clustering/Section%2025%20-%20Hierarchical%20Clustering/Python/Mall_Customers.csv'\n",
        "dataset = pd.read_csv(datloc)\n",
        "\n",
        "X = dataset.iloc[:,[3,4]].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq9raU2mUSj5",
        "outputId": "8af2fe57-96ee-42f3-b03b-0f4bf75211ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 15,  39],\n",
              "       [ 15,  81],\n",
              "       [ 16,   6],\n",
              "       [ 16,  77],\n",
              "       [ 17,  40],\n",
              "       [ 17,  76],\n",
              "       [ 18,   6],\n",
              "       [ 18,  94],\n",
              "       [ 19,   3],\n",
              "       [ 19,  72],\n",
              "       [ 19,  14],\n",
              "       [ 19,  99],\n",
              "       [ 20,  15],\n",
              "       [ 20,  77],\n",
              "       [ 20,  13],\n",
              "       [ 20,  79],\n",
              "       [ 21,  35],\n",
              "       [ 21,  66],\n",
              "       [ 23,  29],\n",
              "       [ 23,  98],\n",
              "       [ 24,  35],\n",
              "       [ 24,  73],\n",
              "       [ 25,   5],\n",
              "       [ 25,  73],\n",
              "       [ 28,  14],\n",
              "       [ 28,  82],\n",
              "       [ 28,  32],\n",
              "       [ 28,  61],\n",
              "       [ 29,  31],\n",
              "       [ 29,  87],\n",
              "       [ 30,   4],\n",
              "       [ 30,  73],\n",
              "       [ 33,   4],\n",
              "       [ 33,  92],\n",
              "       [ 33,  14],\n",
              "       [ 33,  81],\n",
              "       [ 34,  17],\n",
              "       [ 34,  73],\n",
              "       [ 37,  26],\n",
              "       [ 37,  75],\n",
              "       [ 38,  35],\n",
              "       [ 38,  92],\n",
              "       [ 39,  36],\n",
              "       [ 39,  61],\n",
              "       [ 39,  28],\n",
              "       [ 39,  65],\n",
              "       [ 40,  55],\n",
              "       [ 40,  47],\n",
              "       [ 40,  42],\n",
              "       [ 40,  42],\n",
              "       [ 42,  52],\n",
              "       [ 42,  60],\n",
              "       [ 43,  54],\n",
              "       [ 43,  60],\n",
              "       [ 43,  45],\n",
              "       [ 43,  41],\n",
              "       [ 44,  50],\n",
              "       [ 44,  46],\n",
              "       [ 46,  51],\n",
              "       [ 46,  46],\n",
              "       [ 46,  56],\n",
              "       [ 46,  55],\n",
              "       [ 47,  52],\n",
              "       [ 47,  59],\n",
              "       [ 48,  51],\n",
              "       [ 48,  59],\n",
              "       [ 48,  50],\n",
              "       [ 48,  48],\n",
              "       [ 48,  59],\n",
              "       [ 48,  47],\n",
              "       [ 49,  55],\n",
              "       [ 49,  42],\n",
              "       [ 50,  49],\n",
              "       [ 50,  56],\n",
              "       [ 54,  47],\n",
              "       [ 54,  54],\n",
              "       [ 54,  53],\n",
              "       [ 54,  48],\n",
              "       [ 54,  52],\n",
              "       [ 54,  42],\n",
              "       [ 54,  51],\n",
              "       [ 54,  55],\n",
              "       [ 54,  41],\n",
              "       [ 54,  44],\n",
              "       [ 54,  57],\n",
              "       [ 54,  46],\n",
              "       [ 57,  58],\n",
              "       [ 57,  55],\n",
              "       [ 58,  60],\n",
              "       [ 58,  46],\n",
              "       [ 59,  55],\n",
              "       [ 59,  41],\n",
              "       [ 60,  49],\n",
              "       [ 60,  40],\n",
              "       [ 60,  42],\n",
              "       [ 60,  52],\n",
              "       [ 60,  47],\n",
              "       [ 60,  50],\n",
              "       [ 61,  42],\n",
              "       [ 61,  49],\n",
              "       [ 62,  41],\n",
              "       [ 62,  48],\n",
              "       [ 62,  59],\n",
              "       [ 62,  55],\n",
              "       [ 62,  56],\n",
              "       [ 62,  42],\n",
              "       [ 63,  50],\n",
              "       [ 63,  46],\n",
              "       [ 63,  43],\n",
              "       [ 63,  48],\n",
              "       [ 63,  52],\n",
              "       [ 63,  54],\n",
              "       [ 64,  42],\n",
              "       [ 64,  46],\n",
              "       [ 65,  48],\n",
              "       [ 65,  50],\n",
              "       [ 65,  43],\n",
              "       [ 65,  59],\n",
              "       [ 67,  43],\n",
              "       [ 67,  57],\n",
              "       [ 67,  56],\n",
              "       [ 67,  40],\n",
              "       [ 69,  58],\n",
              "       [ 69,  91],\n",
              "       [ 70,  29],\n",
              "       [ 70,  77],\n",
              "       [ 71,  35],\n",
              "       [ 71,  95],\n",
              "       [ 71,  11],\n",
              "       [ 71,  75],\n",
              "       [ 71,   9],\n",
              "       [ 71,  75],\n",
              "       [ 72,  34],\n",
              "       [ 72,  71],\n",
              "       [ 73,   5],\n",
              "       [ 73,  88],\n",
              "       [ 73,   7],\n",
              "       [ 73,  73],\n",
              "       [ 74,  10],\n",
              "       [ 74,  72],\n",
              "       [ 75,   5],\n",
              "       [ 75,  93],\n",
              "       [ 76,  40],\n",
              "       [ 76,  87],\n",
              "       [ 77,  12],\n",
              "       [ 77,  97],\n",
              "       [ 77,  36],\n",
              "       [ 77,  74],\n",
              "       [ 78,  22],\n",
              "       [ 78,  90],\n",
              "       [ 78,  17],\n",
              "       [ 78,  88],\n",
              "       [ 78,  20],\n",
              "       [ 78,  76],\n",
              "       [ 78,  16],\n",
              "       [ 78,  89],\n",
              "       [ 78,   1],\n",
              "       [ 78,  78],\n",
              "       [ 78,   1],\n",
              "       [ 78,  73],\n",
              "       [ 79,  35],\n",
              "       [ 79,  83],\n",
              "       [ 81,   5],\n",
              "       [ 81,  93],\n",
              "       [ 85,  26],\n",
              "       [ 85,  75],\n",
              "       [ 86,  20],\n",
              "       [ 86,  95],\n",
              "       [ 87,  27],\n",
              "       [ 87,  63],\n",
              "       [ 87,  13],\n",
              "       [ 87,  75],\n",
              "       [ 87,  10],\n",
              "       [ 87,  92],\n",
              "       [ 88,  13],\n",
              "       [ 88,  86],\n",
              "       [ 88,  15],\n",
              "       [ 88,  69],\n",
              "       [ 93,  14],\n",
              "       [ 93,  90],\n",
              "       [ 97,  32],\n",
              "       [ 97,  86],\n",
              "       [ 98,  15],\n",
              "       [ 98,  88],\n",
              "       [ 99,  39],\n",
              "       [ 99,  97],\n",
              "       [101,  24],\n",
              "       [101,  68],\n",
              "       [103,  17],\n",
              "       [103,  85],\n",
              "       [103,  23],\n",
              "       [103,  69],\n",
              "       [113,   8],\n",
              "       [113,  91],\n",
              "       [120,  16],\n",
              "       [120,  79],\n",
              "       [126,  28],\n",
              "       [126,  74],\n",
              "       [137,  18],\n",
              "       [137,  83]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czYMlG7cNBsu"
      },
      "source": [
        "## Using the dendrogram to find the optimal number of clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDbXbo9INLF6"
      },
      "source": [
        "## Training the Hierarchical Clustering model on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-SYG7l9NVmU"
      },
      "source": [
        "## Visualising the clusters"
      ]
    }
  ]
}