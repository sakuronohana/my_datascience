{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xg_boost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakuronohana/my_datascience/blob/master/udemy/mlaz/Part%2010%20-%20Model%20Selection%20_%20Boosting/Section%2049%20-%20XGBoost/Python/boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FLud1n-3pVm"
      },
      "source": [
        "# Boosting\n",
        "\n",
        "Der Begriff 'Boosting' bezieht sich auf eine Familie von Algorithmen, die schwache Lerner in starke Lerner umwandeln. Boosting ist eine Ensemble-Methode zur Verbesserung der Modellvorhersagen eines bestimmten Lernalgorithmus. Die Idee des Boosting besteht darin, schwache Lernende nacheinander zu trainieren, wobei jeder versucht, seinen Vorgänger zu korrigieren.\n",
        "\n",
        "<img src='https://miro.medium.com/max/875/0*qCcM7uCOqIw6npnJ.png' width=500>\n",
        "\n",
        "Quelle:  [Quantdare](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/)\n",
        "\n",
        "## eXtreme Gradient Boosting (XGBoost)\n",
        "\n",
        "XGBoost ist eine frei verfügbare Bibliothek mit Open-Source-Lizenz. Sie ermöglicht überwachtes maschinelles Lernen mit dem Boosted-Tree-Algorithmus, einem Decision Tree mit Gradient Boosting. Mithilfe von XGBoost lassen sich Zielvariablen genauer bestimmen, indem mehrere einfachere und schwächere Modelle miteinander kombiniert und Schätzungen getroffen werden. Die Software adressiert Problemstellungen des maschinellen Lernens in den Bereichen Regression und Klassifizierung. XGBoost arbeitet performant und liefert zuverlässige Ergebnisse.\n",
        "\n",
        "Er wird für überwachtes maschinelles Lernen eingesetzt. Boosting ist eine Methode, die verschiedene einfachere und schwächere Modelle kombiniert, um bessere Vorhersagen der Zielvariablen zu machen. Es werden so lange Modelle hinzugefügt, bis keine Verbesserungen der Vorhersagen mehr eintreten. Beim Gradient Boosting kommt ein spezieller Gradienten-Algorithmus, der sogenannte Gradient-Descent-Algorithmus, zum Hinzufügen der Modelle zum Einsatz. Der Baumalgorithmus mit Gradient Boosting fügt neue Verzweigungen hinzu, die Fehler vorheriger Verzweigungen vorhersagen und die Fehler oder Verluste minimieren. Durch Verknüpfung der Bäume beziehungsweise der Verzweigungen entstehen die endgültigen Prognosen.\n",
        "\n",
        "## AdaBoost\n",
        "\n",
        "Neben XGBoost bestehen auch noch AdaBoost (Adaptive Boosting)\n",
        "\n",
        "Adaboost kombiniert mehrere schwache Lernende zu einem einzigen starken Lernenden. Die schwachen Lerner in AdaBoost sind Entscheidungsbäume mit einer einzigen Aufspaltung, den sogenannten Entscheidungsstümpfen. Wenn AdaBoost seinen ersten Entscheidungsstumpf erstellt, werden alle Beobachtungen gleich gewichtet. Um den vorherigen Fehler zu korrigieren, haben die falsch klassifizierten Beobachtungen jetzt mehr Gewicht als die korrekt klassifizierten Beobachtungen. Die AdaBoost-Algorithmen können sowohl für Klassifizierungs- als auch für Regressionsprobleme verwendet werden.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO8VPU6n3vES"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clDSsF7P33NU"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGpwK5XD386E"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcksk88u4Ae8"
      },
      "source": [
        "datloc = 'https://raw.githubusercontent.com/sakuronohana/my_datascience/master/udemy/mlaz/Part%2010%20-%20Model%20Selection%20_%20Boosting/Section%2049%20-%20XGBoost/Python/Data.csv'\n",
        "dataset = pd.read_csv(datloc)\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNn2RnST6_Q-"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajhBL-er7Gry"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y89ctGZ7Mcx"
      },
      "source": [
        "## Training XGBoost on the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ude1J0E47SKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97020b67-3a01-4cb5-fc47-9ec84604c6c2"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "classifier = XGBClassifier()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivqmubzW7dFJ"
      },
      "source": [
        "## Making the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUSZ3zm_7gRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc0b9ee-bf44-4699-abaf-f7f3ba7c46e6"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[84  3]\n",
            " [ 0 50]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9781021897810219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnbCjHgQ8XPn"
      },
      "source": [
        "## Applying k-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbfiITD8ZAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe3b108-7623-463a-c403-a56829a927a5"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
        "print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 96.53 %\n",
            "Standard Deviation: 2.07 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILyFqLXAVXoC"
      },
      "source": [
        "Im Vergleich zu anderen [Klassifikatoren](https://github.com/sakuronohana/my_datascience/tree/master/udemy/mlaz/Part%203%20-%20Classification) hat dieses erweiterte Desicion-Tree Modell eine sehr hohe Genauigkeit erreicht.  "
      ]
    }
  ]
}