{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS Bootcamp Machine Learning Datenbeschaffung Datenaufbereitung ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakuronohana/my_datascience/blob/master/DS_Bootcamp_ML_Datenbeschaffung_Datenaufbereitung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyxBmxgRxnMS",
        "colab_type": "text"
      },
      "source": [
        "# DS Bootcamp Teil 2 - Machine Learning - Datenbeschaffung & Datenaufbereitung #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXkXM0r6HyM7"
      },
      "source": [
        "## DS Bootcamp Verzeichnis ##\n",
        "\n",
        "[**Teil 1 - Data Science - Prozesse & Grundlagen**](https://github.com/sakuronohana/my_datascience/blob/master/DS_Bootcamp_Prozesse_%26_Grundlagen.ipynb)\n",
        "\n",
        "[**Teil 2 - Data Science - ML - Datenbeschaffung & Datenaufbereitung**](https://github.com/sakuronohana/my_datascience/blob/master/DS_Bootcamp_ML_Datenbeschaffung_Datenaufbereitung.ipynb)\n",
        "\n",
        "[**Teil 3 - Data Science - ML - Modellwahl**](https://github.com/sakuronohana/my_datascience/blob/master/DS_Bootcamp_ML_Modellwahl.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_IA7ITTJFce",
        "colab_type": "text"
      },
      "source": [
        "##Datenbeschaffung (Research/Data Gathering)##\n",
        "\n",
        "Die Beschaffung der richtigen Daten zur Erreichung des gesetzten Zieles stellt sich oft als schwieriger heraus als gedacht. Einen Datensatz in der richtigen Qualität und Quantität zu finden ist oft eine Herausforderung. Mit viel Glück bestehen im eigenen Unternehmen grossen Datenmengen die genutzt werden können. Oft müssen diese jedoch noch zusätzlich mit externen Daten angereichert werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpvkX09dU1j4",
        "colab_type": "text"
      },
      "source": [
        "###Datensatzsuche im Internet###\n",
        "\n",
        "Nachfolgend werden ein paar wertvolle Quellen für den Bezug von Datensätzen aufgelistet. Die meisten dieser Datenquellen sind frei zugänglich:\n",
        "\n",
        "*  Google Dataset Search - Ist eine von Google optimierte Engine für die Suche von Datensätzen [Link](https://toolbox.google.com/datasetsearch)\n",
        "*  Google Dataset Selection [Link](https://ai.google/tools/datasets/)\n",
        "*  Google TensorFlow Datasets - Mit dem Module tensorflow_dataset können verschieden Datensätze für das Testen von ML Modellen angezogen werden [Link](https://www.tensorflow.org/datasets/catalog/overview) \n",
        "*  Kaggle Datasets - Die grosse DS Competition Platform Kaggle verfügt über eine Vielzahl von guten Datensätzen [Link](https://www.kaggle.com/datasets)\n",
        "*  U.S. Government’s open data - Datenbank mit ein Vielzahl von Datensätzen ermittelt durch die U.S Regierung [Link](https://www.data.gov/)\n",
        "*  Swiss Open Government Data - Datensätze der Schweizer Regierung [Link](https://opendata.swiss/de/)\n",
        "*  European Bioinformatics Institute - Enthält eine Vielzahl von Daten aus dem Domains Biologie und Chemie [Link](https://www.ebi.ac.uk/services)\n",
        "* Cool Datasets - Sammlung von Datensatzquellen [Link](https://www.cooldatasets.com/)\n",
        "*  Wikipedia List of Datasets - Wikipedia Seite mit einer Vielzahl von Links zu Datensätzen [Link](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)\n",
        "\n",
        "Neben den oben gelisteten Datensätze gibt es noch eine Menge mehr. Am besten jeweils mit folgenden Suchbegriffen in Google suchen:\n",
        "\n",
        "    open dataset for \"Thema\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qNntcDOQxuMI"
      },
      "source": [
        "###Bilder selber herstellen###\n",
        "\n",
        "Neben den unzähligen Datensets welche sich im Internet finden lassen, gibt es auch andere Möglichkeiten Bilder für die das Trainieren eines Klassifikators zu erstellen.\n",
        "\n",
        "***Bilder mit Fotokamera erstellen***\n",
        "\n",
        "Diese Methode gehört wohl zur mühsamsten Art um Bilder selber herzustellen, da es für das Training eines Klassifikators tausende von Bildern braucht.\n",
        "\n",
        "***Bilder aus Video extrahieren***\n",
        "\n",
        "Ein guter Trick um in einer kurzen Zeit zu vielen Bildern zu gelangen ist ein Video eines Gegenstands zu drehen. Die einzelnen Bilder lassen sich dann mit dem Tool ffmpeg aus dem Video extrahieren:\n",
        "\n",
        "   *ffmpeg -i schale.mp4 schale%03d.jpg*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4H8Ea2zyHsNK"
      },
      "source": [
        "###Scraping (Sammeln) von Google Images###\n",
        "\n",
        "Google verfügt mittlerweilen über eine riesige Datenbank von Bildern, welche sich natürlich gut für das Training von Klassifikatoren (Neuronale Netzwerke)eignet.\n",
        "\n",
        "Nachfolgend kurz eine kleines Python-Script für die Sammlung von verschiedenen Bildern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yAXhWVUaF3O7",
        "colab": {}
      },
      "source": [
        "# importing google_images_download module\n",
        "from google_images_download import google_images_download\n",
        "\n",
        "# creating object \n",
        "response = google_images_download.googleimagesdownload()\n",
        "\n",
        "search_queries = ['Dogs', \n",
        "                  'Cats',]\n",
        "  \n",
        "def downloadimages(query): \n",
        "    # keywords is the search query \n",
        "    # format is the image file format \n",
        "    # limit is the number of images to be downloaded \n",
        "    # print urs is to print the image file url \n",
        "    # size is the image size which can \n",
        "    # be specified manually (\"large, medium, icon\") \n",
        "    # aspect ratio denotes the height width ratio \n",
        "    # of images to download. (\"tall, square, wide, panoramic\") \n",
        "    arguments = {\"keywords\": query, \n",
        "                 \"format\": \"jpg\", \n",
        "                 \"limit\":4, \n",
        "                 \"print_urls\":True, \n",
        "                 \"size\": \"medium\"} \n",
        "    try: \n",
        "        response.download(arguments) \n",
        "      \n",
        "    # Handling File NotFound Error     \n",
        "    except FileNotFoundError:  \n",
        "        arguments = {\"keywords\": query, \n",
        "                     \"format\": \"jpg\", \n",
        "                     \"limit\":4, \n",
        "                     \"print_urls\":True,  \n",
        "                     \"size\": \"medium\"} \n",
        "                       \n",
        "        # Providing arguments for the searched query \n",
        "        try: \n",
        "            # Downloading the photos based \n",
        "            # on the given arguments \n",
        "            response.download(arguments)  \n",
        "        except: \n",
        "            pass\n",
        "  \n",
        "# Driver Code \n",
        "for query in search_queries: \n",
        "    downloadimages(query)  \n",
        "    print()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeuHVeVEr04x",
        "colab_type": "text"
      },
      "source": [
        "###Datenformate###\n",
        "\n",
        "Daten können beispielsweise direkt aus Datenbanken (Rationale DB, Polystrukturierte DB usw.) oder mittels Dateien extrahiert werden. Während man im Unternehmen selbst oft auf Datenbanken wie das Data Warehouse (DWH oder IWH) usw. mittels SQL zugreifen und Daten daraus extrahieren kann, sind im Internet verschiedene Dateiformate im Umlauf. Welches Format am Besten ist, ist oft eine philosophische Angelegenheit. Es gibt aber teilweise Unterschiede im Bezug auf die Performance beim Import (bspw. CSVs werden viel schneller geladen als XLSX) Nachfolgend die wichtigsten bzw. beliebtesten Formate (siehe detailierte Beschreibung ([Link](https://towardsdatascience.com/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-2e0c3d18d4f9)):\n",
        "\n",
        "* Columnar Data File Formats (Database-Files)\n",
        "  * .parquet\n",
        "  * .orc\n",
        "  * .petastorm\n",
        "* Tabular Text-based File Formats\n",
        "  * .csv\n",
        "  * .xlsx\n",
        "* Nested File Formats\n",
        "  * .tfrecords\n",
        "  * .json\n",
        "  * .xml\n",
        "  * .avro\n",
        "* Array-Based Formats\n",
        "  * .npy\n",
        "* Hierarchical Data Formats\n",
        " * .h5\n",
        " * .hdf5\n",
        " * .nc\n",
        "* Model File Formats\n",
        " * .pb\n",
        " * .onnx\n",
        " * .pkl\n",
        " * .mlmodel\n",
        " * .zip\n",
        " * .pmml\n",
        " * .pt\n",
        "\n",
        " Nachfolgend die verwendeten Datenformat noch nach Einsatzzweck:\n",
        " <img src=\"https://miro.medium.com/max/715/0*H7CB1kGuukCv2rcK.png\" alt=\"Drawing\" style=\"width: 40px;\"/>\n",
        "  <img src=\"https://miro.medium.com/max/626/0*0MZyp6CdafGNrnUr.png\" alt=\"Drawing\" style=\"width: 40px;\"/>\n",
        "  <img src=\"https://miro.medium.com/max/713/0*phrNmrrcyoX-lnIE.png\" alt=\"Drawing\" style=\"width: 40px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vexp-ZwJWfG",
        "colab_type": "text"
      },
      "source": [
        "##Datenaufbereitung (Data Pre-Processing)##\n",
        "\n",
        "Beim Data Pre-Processing, geht es im Wesentlichen darum, die Daten für das ML-Modell nutzbar zu machen. Was sich hier so einfach anhört ist in der Praxis ein relativ aufwendiger Prozess und benötigt nebst der Datenbeschaffung das grösste Zeitbudget. Das DPP besteht aus folgenden nicht abschliessenden Aktivitäten:\n",
        "\n",
        "* **Datensatzanalyse**. Meist visuelle Erkundung der Art und Struktur des Datensatzes. Oft wird in dieser Phase folgende Punkte ermittelt:\n",
        "  * Datentyp (nummerische oder kategorische Werte)\n",
        "  * Datensatzwerte (oft pro Feature):\n",
        "    * Anzahl Datensätze (Count)\n",
        "    * Anzahl fehlende Werte (Missing)\n",
        "    * Arihmetischer Mittelwert (Mean) -> nur nummerischen Features\n",
        "    * Zentralwert (Median)\n",
        "    * Häufigster Wert (Modus)\n",
        "    * Standardabweichung (Standard Deviation) -> nur nummerischen Features \n",
        "    * Null-Werte (Zeros)\n",
        "    * Kleinster Wert (Min)\n",
        "    * Grösster Wert (Max)\n",
        "    * Anzahl einmaliger Werte (Uniques)\n",
        "  \n",
        "  **Tools:**\n",
        "\n",
        "  Die Datensatzanalyse kann einerseits mittels Python durchgeführt werden. Es bieten sich jedoch hier auch das von Google zur Verfügung gestellte Tool **Facets** ([Link](https://pair-code.github.io/facets/)) an. Mittels Facets lassen sich die Daten aus mehreren Sichten betrachte.\n",
        "\n",
        " <img src=\"https://3.bp.blogspot.com/-lkb4w1DrJ-A/WWzzyPC428I/AAAAAAAAB48/TrSFgqxaYPY-jMv0cmXJaskUz9ImyXxLwCLcBGAs/s1600/image3.png\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
        "  \n",
        " <img src=\"https://raw.githubusercontent.com/PAIR-code/facets/master/img/dive-census.png\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
        "\n",
        "* **Datenbereinigung**. Entfernen oder Korrigieren von Datensätzen mit beschädigten oder ungültigen Werten aus Rohdaten sowie Entfernen von Datensätzen, bei denen eine große Zahl von Spalten fehlt\n",
        "* **Instanzauswahl und Partitionierung**. Auswählen von Datenpunkten aus dem Eingabe-Dataset zum Erstellen von Trainings-, Evaluations- (Validierungs-) und Testdaten. Dieser Prozess umfasst Verfahren für wiederholbare, zufällige Stichproben, die Überabtastung von \"Minderheitsklassen\" und geschichtete Partitionierung.\n",
        "* **Abstimmung von Merkmalen**. Verbessern der Qualität eines Merkmals für ML, einschließlich der Skalierung und Normalisierung numerischer Werte, der Eingabe fehlender Werte, des Ausklammerns von Ausreißern und der Anpassung von Werten mit asymmetrischen Verteilungen\n",
        "* **Darstellungstransformation**. Umwandeln eines numerischen Merkmals in ein kategoriales Merkmal (durch \"Bucketization\") und Umwandeln kategorialer Merkmale in eine numerische Darstellung (durch One-Hot-Codierung, Lernen mit Anzahlen, Einbetten von Merkmalen mit geringer Dichte usw.). Einige Modelle funktionieren nur mit numerischen oder kategorialen Merkmalen, andere können gemischte Merkmale verarbeiten. Auch wenn Modelle beide Typen verarbeiten können, können sie von einer unterschiedlichen Darstellung (numerisch und kategorial) desselben Merkmals profitieren.\n",
        "* **Extraktion von Merkmalen**. Reduzieren der Anzahl von Merkmalen durch das Erstellen von Datendarstellungen mit weniger Dimensionen und höherer Leistungsfähigkeit. Dabei werden Verfahren wie PCA, das Extrahieren von Einbettungen und Hashen eingesetzt.\n",
        "* **Auswahl von Merkmalen**. Auswählen einer Teilmenge der Eingabemerkmale zum Trainieren des Modells und Ignorieren irrelevanter oder redundanter Merkmale mithilfe von Filter- oder Wrapper-Methoden. Das kann auch einfaches Löschen von Merkmalen beinhalten, wenn bei den Merkmalen eine große Zahl von Werten fehlt\n",
        "* **Erstellung von Merkmalen**. Erstellen neuer Merkmale, entweder mithilfe typischer Methoden wie Polynom-Erweiterung (Distributiv-/Verteilungsgesetze) – durch Verwendung univariater mathematischer Funktionen – oder durch Feature Crossing (zum Erfassen von Merkmalsinteraktionen). Merkmale können auch mithilfe von Geschäftslogik aus dem Fachbereich des ML-Anwendungsfalls erstellt werden.\n",
        "\n"
      ]
    }
  ]
}
