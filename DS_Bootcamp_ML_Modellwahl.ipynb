{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Bootcamp ML Modellwahl ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakuronohana/my_datascience/blob/master/DS_Bootcamp_ML_Modellwahl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyxBmxgRxnMS",
        "colab_type": "text"
      },
      "source": [
        "# DS Bootcamp Teil 3 - Machine Learning - Modellwahl #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXkXM0r6HyM7"
      },
      "source": [
        "## DS Bootcamp Verzeichnis ##\n",
        "\n",
        "[**Teil 1 - Data Science - Prozesse & Grundlagen**](https://github.com/sakuronohana/my_datascience/blob/master/DS_Bootcamp_Prozesse_%26_Grundlagen.ipynb)\n",
        "\n",
        "[**Teil 2 - Data Science - ML - Datenbeschaffung & Datenaufbereitung**](https://github.com/sakuronohana/my_datascience/blob/master/DS_Bootcamp_ML_Datenbeschaffung_Datenaufbereitung.ipynb)\n",
        "\n",
        "[**Teil 3 - Data Science - ML - Modellwahl**]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmdDPoLuJqn5",
        "colab_type": "text"
      },
      "source": [
        "##Modellwahl (Choose Model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6PRLqEhXBHC"
      },
      "source": [
        "##Machine Learning Modelle##\n",
        "\n",
        "In diesem Bootcamp beschäftigen wir uns mit den verschiedenen Methoden und Modelle des Maschine Learnings. \n",
        "\n",
        "![alt text](https://blogs.sas.com/content/subconsciousmusings/files/2017/04/machine-learning-cheet-sheet.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWDuVk1e2Dlm"
      },
      "source": [
        "####K Nearest Neighbours (KNN)####\n",
        "\n",
        "KNN gehört wohl zu den einfachsten ML-Algorithmen und wird sowohl für die Klassifizierung sowie Regression verwendet. Es handelt sich dabei um eine sogenannte parameterfreie Methode zur Schätzung von Wahrscheinlichkeitsdichtefunktionen (eng. probability density function / pdf). KNN arbeitet nach dem Klassifikationsverfahren, in welchem eine Klassenzuordnung unter Berücksichtigung sein ***k*** nächsten Nachbarn vorgenommen wird. Das Modell lernt dabei mittels Abspeicherung der Trainingsbeispielen was auch ***\"lazy learning\"*** (träges Lernen) genannt wird. KNN wird überall dort eingesetzt wo es darum geht Daten mit gleichartigen Mustern zu finden. Beispiele dafür sind Spam, Krebszellen usw. \n",
        "\n",
        "![KNNt](https://www.fromthegenesis.com/wp-content/uploads/2018/09/K_NN_Ad.jpg)\n",
        "\n",
        "[Weiterführende Infos unter Wikipedia](https://de.wikipedia.org/wiki/N%C3%A4chste-Nachbarn-Klassifikation)\n",
        "\n",
        "\n",
        "**KNN Step-by-Step**\n",
        "\n",
        "Im wesentlichen geht es im KNN darum die  Anzahl der ***k*** zu definieren, damit eine optimale Klassenzuordnung gemacht werden kann und die Distanz zwischen zu den nächsten Datenpunkte zu berechnen. \n",
        "\n",
        "Zu Berechnung der Distanz stehen im KNN drei Methoden zur Verfügung:\n",
        "\n",
        "![alt text](https://www.saedsayad.com/images/KNN_similarity.png)\n",
        "\n",
        "Am Beispiel eines Stadtplans ist die Euclidean Methode in Grün und Manhatten in blau:\n",
        "\n",
        "![alt text](https://qph.fs.quoracdn.net/main-qimg-e73d01f18d0b4a2f57ff2206a3863c10.webp)\n",
        "\n",
        "\n",
        "**Step 1 - Berechnung der (Euclidean) Distance\n",
        "\n",
        "Nehmen wir an, dass wir einen Datensatz bestehend aus Körpergrösse, Gewicht und T-Shirt Grösse haben:\n",
        "\n",
        "Grösse in cm (x) | Gewicht in  kg (y) | T-Shirt Grösse | Euclidean Distance\n",
        "--- | --- | --- \n",
        "158 | 58 | M\n",
        "158 | 59 | M\n",
        "158 | 63 | M\n",
        "160 | 59 | M\n",
        "160 | 60 | M\n",
        "163 | 60 | M\n",
        "163 | 61 | M\n",
        "160 | 64 | L\n",
        "163 | 64 | L\n",
        "165 | 61 | L\n",
        "165 | 62 | L\n",
        "165 | 65 | L\n",
        "168 | 62 | L\n",
        "168 | 63 | L\n",
        "168 | 66 | L\n",
        "170 | 63 | L\n",
        "170 | 64 | L\n",
        "170 | 68 | L\n",
        "\n",
        "Die neuen Daten sind:\n",
        "\n",
        "Grösse in cm (x) | Gewicht in  kg (y) |\n",
        "--- | --- \n",
        "161 | 62 \n",
        "\n",
        "Mit Hilfe von KKN und der Euclidean Methode wollen wir nun herausfinden zu welche T-Shirt Grösse diese Daten passen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BkhtMBHU2WkQ",
        "colab": {}
      },
      "source": [
        "#KKN - ohne SciKit-Learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#So lassen sich übrigends lässtige Fehlermeldungen unterdrücken\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "data_url = \"https://raw.githubusercontent.com/sakuronohana/my_datascience/master/kkn_t-shirt.csv\"\n",
        "\n",
        "data = pd.read_csv(data_url,delimiter='|')\n",
        "data.columns=['Groesse(cm)','Gewicht(kg)','T-Shirt']\n",
        "df_data_x = np.int32(np.ravel(pd.DataFrame(data['Groesse(cm)'])))\n",
        "df_data_y = np.int32(np.ravel(pd.DataFrame(data['Gewicht(kg)'])))\n",
        "new_data = pd.DataFrame([[161,62]], columns=['Groesse(cm)','Gewicht(kg)'])\n",
        "df_new_data_x = np.int32(np.ravel(pd.DataFrame(new_data['Groesse(cm)'])))\n",
        "df_new_data_y = np.int32(np.ravel(pd.DataFrame(new_data['Gewicht(kg)'])))\n",
        "\n",
        "#Berechnung der Distanz nach der euklidischen Distanz Methode\n",
        "df_sqt_xx = (np.subtract(df_new_data_x,df_data_x))**2\n",
        "df_sqt_yy = np.subtract(df_new_data_y,df_data_y)**2\n",
        "dis_euc = np.sqrt(np.add(df_sqt_xx,df_sqt_yy))\n",
        "df_dis_euc = pd.DataFrame(dis_euc, columns=['Distanz'])\n",
        "df_dis_euc.sort_values(by=['Distanz'])\n",
        "\n",
        "df_data_merge = pd.concat([data,df_dis_euc], axis=1)\n",
        "\n",
        "#Wahl der nächsten 5 Nachbarn\n",
        "k_detect = np.where(df_data_merge['Distanz']<3)\n",
        "k_select = df_data_merge.loc[k_detect]\n",
        "k_find = k_select['T-Shirt'].value_counts()\n",
        "print('Mit der Grösse von 161 cm und einem Gewicht von 62kg wird \\n '\n",
        "      'wahrscheinlich die T-Shirt Grösse%s passen' %(np.argmax(k_find)))\n",
        "\n",
        "\n",
        "ax = data.plot(kind='scatter',x='Gewicht(kg)',y='Groesse(cm)')\n",
        "new_data.plot(ax=ax,kind='scatter',x='Gewicht(kg)',y='Groesse(cm)')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I_0f-toVq9ZN"
      },
      "source": [
        "###Lineare Regression###\n",
        "\n",
        "**Lernverfahren:** Supervised Learning\n",
        "\n",
        "**Lernarten:** Classification & Regression\n",
        "\n",
        "Die lineare Regression gehört zu den einfachsten der Regressionen und orientiert sich, wie der Name schon sagt, an einer Gerade. Mit diesem Modell können Daten mit einfachen Strukturen verarbeitet werden.\n",
        "\n",
        "\n",
        "Lernvideo: https://www.youtube.com/watch?v=E5RjzSK0fvY\n",
        "\n",
        "Mit RL können wir beispielsweise folgende Fragen beantworten:\n",
        "\n",
        "\n",
        "*  Wie wird sich der Kurs der Aktion XY in den nächsten Tagen entwickeln?\n",
        "*  Mit welchen Temperaturen müssen wir in den nächsten Tagen rechnen?\n",
        "* Wie wirkt sich die Temperatur auf den Bierkonsum aus? :-)\n",
        "* Wie wirken sich die Lernstunden auf den Kaffeekonsum aus?\n",
        "* Usw.\n",
        "\n",
        "Mathematisch gesehen versuchen wir folgende Formel zu lösen:\n",
        "\n",
        "**y = mx + b**\n",
        "\n",
        "**y** = Abhängigen Variablen (z.B. Anzahl konsumierter Kaffeetassen pro Tag)\n",
        "\n",
        "**x** = Unabhängigen Variabelen (z.B. Tage, Stunden usw.)\n",
        "\n",
        "**m** = Slope (Gradient = Steigung oder Gefälle)\n",
        "\n",
        "**b** = Y-Intercept \n",
        "\n",
        "Die Slope oder einfach gesagt die Linie, welche aufzeigt ob sich etwas positiv oder negativ entwickelt und gilt als Best Fit Line. Hierfür verwenden wir folgende Formel:\n",
        "\n",
        " m = $\\frac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2}$\n",
        " \n",
        "Gehen wir mal Schritt für Schritt durch den LR Erstellungsprozess:\n",
        "\n",
        "Fragestellung: Wieviele Tassen Kaffee werde ich in den nächsten Tag noch komsumieren.\n",
        "\n",
        "**Step 1 - Berechnung des arithmetisches Mittel (mean)**\n",
        "\n",
        "Berechnen wir zuerst das arithmetische Mittel oder auch anders gesagt den Durchschnittswert von **x** und **y**:\n",
        "\n",
        " $\\bar{x}=\\frac{\\sum(x_i...x_i)}{x_i}$ \n",
        " \n",
        " $\\bar{y}= \\frac{\\sum(y_i...y_i)}{y_i}$\n",
        " \n",
        " Was in diesem Fall folgende Werte ergeben:\n",
        "\n",
        "Tage (x) | Tassen (y)\n",
        "--- | ---\n",
        "1 | 3\n",
        "2 | 4\n",
        "3 | 2\n",
        "4 | 4\n",
        "5 | 5\n",
        "**3 **($\\bar{x})$ | **3.6** ($\\bar{y})$\n",
        "\n",
        "**Step 2 - Berechnung der Werte x und y zum arithmetischen Mittel**\n",
        "\n",
        "Damit wir irgendwann **m** erhalten müssen wir die Distanz der einzelnen Werte von **x** und **y** gegenüber $\\bar{x}$ und $\\bar{y}$ berechnen. Gleichzeitig werden wir die einzelnen Werte jeweils zusammenzählen. Formel:\n",
        "\n",
        "${x}-\\bar{x}$ und  ${y}-\\bar{y}$\n",
        "\n",
        "Tage (x) | Tassen (y) | ${x}-\\bar{x}$ | $y-\\bar{y}$\n",
        "--- | --- | --- | ---\n",
        "1 | 3 | -2 | -0.6\n",
        "2 | 4 | -1 | 0.4\n",
        "3 | 2 | 0 | -1.6\n",
        "4 | 4 | 1 | 0.4\n",
        "5 | 5 | 2 | 1.4\n",
        "**3 **($\\bar{x})$ | **3.6** ($\\bar{y})$\n",
        "\n",
        "**Step 3 - Berechnung von Least-square (kleinste Quadrate)**\n",
        "\n",
        "Damit wir nun die eigendliche Abweichung der Datenpunkt zur Best Fit Line erhalten müssen wir x  noch quadrieren. Hier die Formel dazu:\n",
        "\n",
        "$({x}-\\bar{x})^2$ \n",
        "\n",
        "Da wir aber die Gesamtabweichung ermitteln wollen, werden wir die Werte gleich aufsummieren:\n",
        "\n",
        "Tage (x) | Tassen (y) | ${x}-\\bar{x}$ | $y-\\bar{y}$ | $({x}-\\bar{x})^2$ \n",
        "--- | --- | --- | --- | ---\n",
        "1 | 3 | -2 | -0.6 | 4 \n",
        "2 | 4 | -1 | 0.4 | 1\n",
        "3 | 2 | 0 | -1.6 | 0\n",
        "4 | 4 | 1 | 0.4 | 1\n",
        "5 | 5 | 2 | 1.4 | 4\n",
        "**3 **($\\bar{x})$ | **3.6** ($\\bar{y})$ ||| **10**\n",
        "\n",
        "**Step 4 - Berechnung Gesamtsumme von x und y**\n",
        "\n",
        "Wir müssen nun noch die Werte aus ${x} -\\bar{x}$  und  ${y}-\\bar{y}$ multiplzieren.\n",
        "\n",
        "Tage (x) | Tassen (y) | ${x}-\\bar{x}$ | $y-\\bar{y}$ | $({x}-\\bar{x})^2$ | $(x-\\bar{x})(y-\\bar{y})$\n",
        "--- | --- | --- | --- | --- | ---\n",
        "1 | 3 | -2 | -0.6 | 4 | 1.2\n",
        "2 | 4 | -1 | 0.4 | 1 | -0.4\n",
        "3 | 2 | 0 | -1.6 | 0 | 0\n",
        "4 | 4 | 1 | 0.4 | 1 | 0.4\n",
        "5 | 5 | 2 | 1.4 | 4 | 2.8\n",
        "**3 **($\\bar{x})$ | **3.6** ($\\bar{y})$ ||| **10**|**4**\n",
        "\n",
        "**Step 5 - Berechnung von m**\n",
        "\n",
        "Nun haben wir alle Informationen zusammen um **m** zu berechnen. Nachfolgendend die Berechnung:\n",
        "\n",
        "m = $\\sum\\frac{(x-\\bar{x})(y-\\bar{y})}{(x-\\bar{x})^2}=\\sum\\frac{4}{10}=0.4$\n",
        "\n",
        "\n",
        "**Step 6 - Berchnung von b bzw. Y-Intercept**\n",
        "\n",
        "Nochmal zur Erinnerung. Wir wollen folgende Gleichung lösen:\n",
        "\n",
        "**y = mx+b**\n",
        "\n",
        "Wir haben nun also in den letzen paar Schritten folgende Werte errechnet:\n",
        "\n",
        "**y** = 3.6\n",
        "\n",
        "**m** = 0.4\n",
        "\n",
        "**x** = 3\n",
        "\n",
        "Wir erhalten nun also folgende Gleichung welche es zu lösen gibt:\n",
        "\n",
        "3.6 = 0.4 * 3 + c\n",
        "          \n",
        "3.6 = 1.2 + c\n",
        "\n",
        "3.6 - 1.2 = 2.4 ist somit  **c= 2.4**\n",
        "\n",
        "\n",
        "**Step 7 - Berechnung der Regressionslinie**\n",
        "\n",
        "Nun haben wir alle Informationen zusammen um die Best Fit Linie zu errechnen bzw. die Prediction:\n",
        "\n",
        "m = 0.4\n",
        "c = 2.4\n",
        "y = 0.4x + 2.4\n",
        "\n",
        "**Hinweis:** y sind in diesem Fall die Datenpunkte für die Regressionslinie\n",
        "\n",
        "Wir werden jetzt mittels folgender Formel die Line errechnen:\n",
        "\n",
        "y=mx+b\n",
        "\n",
        "Tage (x) | Tassen (y) | ${x}-\\bar{x}$ | $y-\\bar{y}$ | $({x}-\\bar{x})^2$ | $(x-\\bar{x})(y-\\bar{y})$ | $y_p$|\n",
        "--- | --- | --- | --- | --- | --- | ---\n",
        "1 | 3 | -2 | -0.6 | 4 | 1.2 | 2.8\n",
        "2 | 4 | -1 | 0.4 | 1 | -0.4 | 3.2\n",
        "3 | 2 | 0 | -1.6 | 0 | 0 | 3.6\n",
        "4 | 4 | 1 | 0.4 | 1 | 0.4 | 4.0\n",
        "5 | 5 | 2 | 1.4 | 4 | 2.8 | 4.4\n",
        "\n",
        "\n",
        "Wenn wir nun diese Koordinaten in der Matrix eintragen , dann erhalten wir die Regressionslinie.\n",
        "\n",
        "**Tutorial**: [Machine Learning Tutorial Part - 1 (ab 17:10)](https://www.youtube.com/watch?v=DWsJc1xnOZo&feature=youtu.be&list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Wf-VRQa2830",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dp = [[1,3],[2,4],[3,2],[4,4],[5,5]]\n",
        "rl = [[2.8],[3.2],[3.6],[4.0],[4.4]]\n",
        "\n",
        "plt.plot(dp,'ro', rl, 'bo', rl)\n",
        "plt.title('Linear Regression')\n",
        "plt.axis([0,5,0,6])\n",
        "plt.xlabel('Tage')\n",
        "plt.ylabel('Kaffeetassen')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e3_8MMnX3TqW"
      },
      "source": [
        "**Step 8 - R Squared**\n",
        "\n",
        "In den vorgängigen Schritten haben wir die Regressionslinie gesucht welche am besten fittet. \n",
        "Wir wollen nun aber sehen wie gut unter Modell wirklch fittet. Dazu können wir R Square verwenden. Es geht beim R Square eigendlich darum den Abstand des tatsächlichen arithmetischen Mittels dem Abstand des vorhergesagten arithmetischen Mittels gegenüber zu stellen. $R^2$ wird nur bei der linearen Regression verwendet.\n",
        "\n",
        "Dir Formel für R Square ($R^2$) sieht entsprechend wie folgt aus:\n",
        "\n",
        "$R^2$ = $\\frac{\\sum(y_p\\bar{y})^2}{\\sum(y-\\bar{y})^2}$\n",
        "\n",
        "\n",
        "$(y_p - \\bar{y})^2$ = Distance predicted-mean\n",
        "\n",
        "$(y-\\bar{y})^2$ = Distance actual-mean\n",
        "\n",
        "Tage (x) | Tassen (y) | $y-y$ | $(y-y)^2$ | $y_p$| $(y_p-\\bar{y})$|$(y_p-\\bar{y})^2$\n",
        "--- | --- | --- | --- | --- | --- | --- | ---\n",
        "1 | 3 | -0.6 |  0.36 | 2.8 | -0.8 | 0.64\n",
        "2 | 4 |  0.4 |  0.16 | 3.2 | -0.4 | 0.16\n",
        "3 | 2 |-1.6 |  2.56 | 3.6 | 0 | 0\n",
        "4 | 4 |  0.4 |  0.16 | 4.0 | 0.4 | 0.16\n",
        "5 | 5 | 1.4 | 1.96| 4.4 | 0.8 | 0.64\n",
        "|**3.6**||**5.2**|||**1.6**\n",
        "\n",
        "Da wir nun die Werte haben können wir sie in der Formel eintragen:\n",
        "\n",
        "$R^2$ = $\\frac{1.6}{5.2}$=$\\frac{\\sum(y_p\\bar{y})^2}{\\sum(y-\\bar{y})^2}$=0.3\n",
        "\n",
        "Wir haben somit also einen $R^2$ Value von 0.3. Dieser Wert zeigt uns, dass die Datenpunkte nicht wirklich gut fitten bzw. weit auseinder stehen und keine gute Beziehung aufweisen. Je näher der Wert Richtung 1 rückt desto besser ist der Fit bzw. unser Modell geignet die Daten abzubilden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BYYU4fKk3arU",
        "colab": {}
      },
      "source": [
        "#Lineare Regression - ohne SciKit-Learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''Wir erstellen zuerst zwei Dataframes mit jeweils einem 1D Array mit \n",
        "Temperaturen und Tagen'''\n",
        "\n",
        "list = [[1,3],[2,4],[3,2],[4,4],[5,5]]\n",
        "\n",
        "df_kt = pd.DataFrame(list, columns=['Tage(x)', 'Tassen(y)'])\n",
        "df_kt_x = np.ravel(pd.DataFrame(df_kt['Tage(x)']))\n",
        "df_kt_y = np.ravel(pd.DataFrame(df_kt['Tassen(y)']))\n",
        "\n",
        "\n",
        "#Step 1 - Berechnung des arithmetisches Mittel (mean)\n",
        "\n",
        "x_mean = np.mean(df_kt_x)\n",
        "y_mean = np.mean(df_kt_y)\n",
        "\n",
        "\n",
        "#Step 2 - Berechnung der Werte x und y zum arithmetischen Mittel\n",
        "\n",
        "x_meanx = np.subtract(df_kt_x, x_mean)\n",
        "y_meany = np.subtract(df_kt_y, y_mean)\n",
        "\n",
        "#Step 3 - Berechnung von Least-square (kleinste Quadrate)\n",
        "\n",
        "xsqt = np.sum(np.int32(np.square(x_meanx)))\n",
        "\n",
        "#Step 4 - Berechnung Gesamtsumme von x und y\n",
        "\n",
        "xy_multiply = np.int32(np.sum(np.multiply(x_meanx,y_meany)))\n",
        "\n",
        "#Step 5 - Berechnung von m\n",
        "\n",
        "m_slope = np.divide(xy_multiply,xsqt)\n",
        "\n",
        "#Step 6 - Berechnung von b bzw. Y-Intercept\n",
        "\n",
        "y_intcept = y_mean-(m_slope * x_mean)\n",
        "\n",
        "#Step 7 - Berechnung der Regressionslinie bzw. Datenpunkte\n",
        "\n",
        "regline = np.add(np.multiply(m_slope,df_kt_x),y_intcept)\n",
        "\n",
        "#Step 8 - R Squared\n",
        "\n",
        "sqt_rl_meany = np.sum(np.square(np.subtract(regline,y_mean)))\n",
        "sqt_y_meany = np.sum(np.square(np.subtract(df_kt_y,y_mean)))\n",
        "r_square = round(np.divide(sqt_rl_meany,sqt_y_meany),1)\n",
        "\n",
        "print('Der R Squared Wert liegt bei dieser Linearen Regression bei',r_square, \n",
        "     '\\n Das Modell fittet somit nicht wirklich gut, da der perfekte Fit \\n bei 1 liegen würde')\n",
        "\n",
        "plt.title('Kaffeetassen Plot')\n",
        "plt.plot(list,'bo',regline,'ro',regline)\n",
        "plt.axis([-0.1,5,0,5.5])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ctwnja3s3htW",
        "colab": {}
      },
      "source": [
        "#Lineare Regression - mit SciKit-Learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''Wir erstellen zuerst zwei Dataframes mit jeweils einem 1D Array mit \n",
        "Temperaturen und Tagen'''\n",
        "\n",
        "list = [[1,3],[2,4],[3,2],[4,4],[5,5]]\n",
        "\n",
        "df_kt = pd.DataFrame(list, columns=['Tage(x)', 'Tassen(y)'])\n",
        "df_kt_x = pd.DataFrame(df_kt['Tage(x)'])\n",
        "df_kt_y = pd.DataFrame(df_kt['Tassen(y)'])\n",
        "\n",
        "# Erstellen von Objekt für die Lineare Regression\n",
        "lr_obj = linear_model.LinearRegression()\n",
        "\n",
        "# Trainieren des Models\n",
        "lr_obj.fit(df_kt_x, df_kt_y)\n",
        "\n",
        "# Zeit um mal eine Voraussage (Prediction) zu wagen.\n",
        "reg_y_pred = lr_obj.predict(df_kt_x)\n",
        "reg_y_pred\n",
        "\n",
        "# Die nachfolgenden Codes machen werden im Normalfall mit dem Testset \n",
        "# gemacht was wir aber für dieses Beispiel nicht haben.\n",
        "\n",
        "# Der Koeffizient im nachfolgenden Beispiel stellt die Variable m dar\n",
        "print('Der Koeffizient dieses Modells ist' ,lr_obj.coef_)\n",
        "\n",
        "# Der Mean Square Error (MSE) beschreibt die Abweichung (Error) zwischen\n",
        "# den, durch die Daten, gegebenen Werten (y) und der geschätzten Werte \n",
        "# (Y Prediction)\n",
        "print('Der MSE ist: %2f' % mean_squared_error(df_kt_y,reg_y_pred))\n",
        "\n",
        "# Der nachfolgenden Varianzwert macht im Normalfall nur \n",
        "print('Der Varianzwert: ', r2_score(df_kt_y,reg_y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YsdLVslBX-EQ"
      },
      "source": [
        "###Support Vector Machine (SVM)###\n",
        "\n",
        "**Lernverfahren:** Supervised Learning\n",
        "\n",
        "**Lernarten:** Classification & Regression\n",
        "\n",
        "**Wichtige Begriffe:** \n",
        "\n",
        "* Hyperebene = Ist die Trennfläche welche die verglichenen Klassen trennen.\n",
        "* Trennlinie (Hyperplane) = Ist die Mittellinie in der Hyperebene\n",
        "* Stützvektoren (Support Vectors) = Sind die Datenpunkte, welche der Trennlinie am nächsten sind.\n",
        "* Kernel-Trick = Ist der Parameter mit welchem man nicht-lineare Daten im SVM verwendet kann.\n",
        "* Maximum Margin = Abstand zwischen positiven (richtigen) und negativen (falschen) Support Vectors. Flächen links und rechts des Hyperplane.\n",
        "* Regularization parameter = Ist der Parameter (C) mit welchem die Fehleranfälligkeit definiert werden kann\n",
        "\n",
        "**Tutorials:**\n",
        "[How Support Vector Machine Works by Simplilearn](https://www.youtube.com/watch?v=TtKF996oEl8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DAyBYZSjbTcO"
      },
      "source": [
        "###SVM with Python###\n",
        "\n",
        "Im folgenden Beispiel verwenden wir ein einfache lineare SVM  und nutzen dazu das ML-Modul scikit-learn in Pyhon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wj1QbfoviJMt",
        "colab": {}
      },
      "source": [
        "# Vereinfachte Version\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "\n",
        "# Erstellung von Datenpunkten\n",
        "# Hier erstellen wir 40 Datenpunkte und versammeln jeweils 20 Punkt in  \n",
        "# 2 Datenpunktehaufen\n",
        "X, y = make_blobs(n_samples=40, centers=2, random_state=20)\n",
        "print(X)\n",
        "\n",
        "# wir fitten nun das Modell \n",
        "\n",
        "clf = svm.SVC(kernel='linear', C=1) # C definiert wieviele Fehler wir zulassen\n",
        "clf.fit(X,y)\n",
        "\n",
        "# Visualisierung des Modells\n",
        "plt.scatter(X[:,0], X[:,1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "gCbl3h1ciJFY",
        "colab": {}
      },
      "source": [
        "# Bringen wir nun zweit neue Datenpunkt ins Spiel\n",
        "\n",
        "newData = [[3,4],[6,8]]\n",
        "print (clf.predict(newData))\n",
        "\n",
        "'''Das Resultat zeigt uns, dass ein Datenpunkt zur \n",
        "Gruppe 0 und einer zur Gruppe 1 zugeordnet wurde''' \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BzWYnu9PtGEC",
        "colab": {}
      },
      "source": [
        "# Fortgeschrittene Version - Blick hinter die Kulissen\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "\n",
        "# Erstellung von Datenpunkten\n",
        "# Hier erstellen wir 40 Datenpunkte und versammeln jeweils 20 Punkt in  \n",
        "# 2 Datenpunktehaufen\n",
        "X, y = make_blobs(n_samples=40, centers=2, random_state=20)\n",
        "\n",
        "\n",
        "# wir fitten nun das Modell \n",
        "clf = svm.SVC(kernel='linear', C=1000) # C definiert wieviele Fehler wir zulassen\n",
        "clf.fit(X,y)\n",
        "\n",
        "# Visualisierung des Modells\n",
        "plt.scatter(X[:,0], X[:,1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "\n",
        "# plot the decision function\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "# Create grid to evaluate model\n",
        "'''Nachfolgend definieren wir den Large-Margin-Classificator bzw. \n",
        "Fläche, welche die zwei Datenpunktgruppen voneinander abgrenzt'''\n",
        "xx = np.linspace(xlim[0], xlim[1], 30) #Der Wert 30 definiert die Anzahl Striche\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "Z = clf.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# plot decision boundary and margins\n",
        "ax.contour(XX,YY, Z, colors='k', levels=[-1,0,1], alpha=0.5,\n",
        "           linestyles=['--','-','--'])\n",
        "# plot support vectors\n",
        "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:,1],\n",
        "           s=100, linewidth=1, facecolors='none')\n",
        "\n",
        "print('Krokodile (rote Punkte) haben eine längere Schnauze während \\n Alligatoren (hellblaue Punkte) grösser sind')\n",
        "plt.xlabel('Schnauzenlänge')\n",
        "plt.ylabel('Körperlänge')\n",
        "plt.title('Alligator oder Krokodil?')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DgAmsBN1Fj_X"
      },
      "source": [
        "###K-Means###\n",
        "\n",
        "**Lernverfahren:** Unsupervised Learning\n",
        "\n",
        "**Lernarten:** Clustering (Partitional Clustering)\n",
        "\n",
        "Der **k-Means-Algorithmus** ist ein Verfahren zur Vektorquantisierung, welches auch zur Clusteranalyse verwendet wird. Dabei wird aus einer Menge von ähnlichen Objekten eine vorher bekannte Anzahl von k Gruppen gebildet. Der Algorithmus ist eine der am häufigsten verwendeten Techniken zur Gruppierung von Objekten, da er schnell die Zentren der Cluster findet. Dabei bevorzugt der Algorithmus Gruppen mit geringer Varianz bzw. Distanz und ähnlicher Größe.\n",
        "\n",
        "k-Means Clustering funtioniert in etwa wie folgt:\n",
        "\n",
        "![alt text](https://image.slidesharecdn.com/kmeansclusteringalgorithmkmeansclusteringexamplemachinelearningalgorithmssimplilearn-180323072616/95/k-means-clustering-algorithm-k-means-clustering-example-machine-learning-algorithms-simplilearn-32-1024.jpg?cb=1521790168)\n",
        "\n",
        "Nehmen wir an, das wir die Inputs x1, x2, x3 .... und den K-Wert,\n",
        "\n",
        "1. Schritt: Zufallsauswahl von k als Cluster Center (Centroids) *Hinweis: Centroids befinden sich nicht wirklich im Cluster-Center*\n",
        "Mathematisch sieht das wie folgt aus, wobei *C* die Summe aller Centroids und $c_n$ jeden einzelnen Centroid darstellt:\n",
        "\n",
        "      $C= c_1,c_2,...c_k$\n",
        "      \n",
        "2. Schritt:  Zuweisung jedes xi zum nächsten Cluster durch die Berechnung der Distanz zu jedem Centroid. Bspw. x1 zu c1, c2 und c4.\n",
        "\n",
        "    $arg  min  dist (c_i, x)^2$\n",
        "    \n",
        "    $^C_i \\in C$\n",
        "\n",
        "3. Schritt: Suchen eines neuen Cluster Centers mittels arithmetischen Mittels der bereits zugewiesenen Datenpunkte.\n",
        "\n",
        "4. Schritt: Wiederholen der Schritte 2 und 3 bis sich die Positionen der Centroids nicht mehr verändern.\n",
        "\n",
        "\n",
        "Zur Ermittlung der Distanzen werden die gleichen Distanzfunktionen (Euc) eingesetzt wie bei KNN (siehe)\n",
        "\n",
        "**Wichtige Begriffe:** \n",
        "\n",
        "* Unlabeled Daten = Geben in Gegensatz zu gelabelten Daten keine Lösung vor. \n",
        "* K = Anzahl Datenpunkte die gewählt werden um Cluster zu bilden.\n",
        "* Cluster centroids = Nennt man die mit K ausgewählten Datenpunkte.\n",
        "\n",
        "**Tutorial Video** [KMeans Clustering Algorithmus](https://www.youtube.com/watch?v=Xvwt7y2jf5E&feature=youtu.be)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GY5mys6cWlAk",
        "colab": {}
      },
      "source": [
        "# K-Means Clustering\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.dataset.samples_generator import make_blobs\n",
        "\n",
        "X, y_true = make_blobs(n_sample=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=S0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "alEfGaoVXHp3",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "data_source = 'https://raw.githubusercontent.com/sakuronohana/my_datascience/master/car_data.csv' \n",
        "dataset = pd.read_csv(data_source, delimiter=';')\n",
        "X = dataset[:-1]\n",
        "X.head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e4sUcxHFPvVG",
        "colab": {}
      },
      "source": [
        "# Datenbereinigung \n",
        "# Konvertierung der Daten in nummerische Werte\n",
        "X = dataset[dataset.columns[:-1]]\n",
        "X = X.convert_objects(convert_numeric=True)\n",
        "\n",
        "# Löschung von Null-Werten\n",
        "for i in X.columns:\n",
        "#Alle Felder Null in den Spalten werden ersetzt mit einem arithmetischen Wert\n",
        "  X[i] = X[i].fillna(int(X[i].mean()))\n",
        "for i in X.columns:\n",
        "  print(X[i].isnull().sum()) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sf7ykmO6Nzgo",
        "colab": {}
      },
      "source": [
        "# Die nachfolgend verwendete Methode zur Findung des optimalen Clusters\n",
        "# heisst Elbow Method (Ellbogenmethode). Der Ellbogen bzw. Knick in der \n",
        "# Kurve wird verwendet um die optimale Anzahl Cluster zu finden.\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Nun erstellen wir einen leeren Array names 'Within sum of Squares (WSS)'\n",
        "wcss = [] \n",
        "print(np.shape(X))\n",
        "\n",
        "for i in range(1,12): #Wir trainieren unsere K-Means Algorthmus 11 mal\n",
        "  kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10,random_state=0)\n",
        "  kmeans.fit(X)\n",
        "  wcss.append(float(i))\n",
        "  \n",
        "  plt.plot(range(1,12), wcss)\n",
        "  plt.title('K-Means Elbow Method')\n",
        "  plt.xlabel('Number of Clusters')\n",
        "  plt.ylabel('WCSS')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0OMRqL-5eaqu"
      },
      "source": [
        "###Deep Learning###\n",
        "\n",
        "Deep Learning ist ein Teilgebiet des Machine Learning und unterscheidet sich vom „klassischen“ maschinellen Lernen, indem es Maschinen in die Lage versetzt, über die verfügbaren Daten hinaus zu lernen. Das beinhaltet die Fähigkeit, Informationen zu analysieren und zu bewerten, um logische Schlüsse zu ziehen, Lösungswege auszuwählen und aus Fehlern zu lernen. Je mehr Daten eine Maschine also empfängt, desto grösser ist ihre Lernfähigkeit und desto \"intelligenter\" kann sie werden. Obwohl es die künstlichen neuronalen Netze, die die Grundlage dieser Technologien bilden, bereits seit den 1950er Jahren gibt, haben erst die bahnbrechenden Entwicklungen des letzten Jahrzehnts die Lernkurve stark verbessert. Die am meisten verbreiteten modernen Applikationen sind Stimm- und Bilderkennung. Das Niveau der Datenanalyse ermöglicht jedoch viele vorausschauende Applikationen, wie enorme Verbesserungen in der vorausschauenden Wartung, sicherere autonome Fahrzeuge, die Vorhersage von Krankheiten oder Rückfällen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dzVwJdUyh8zn"
      },
      "source": [
        "###Neuronale Netzwerke###\n",
        "\n",
        "Ein künstliches neuronales Netzwerk (KNN), im englischen Artificial neural networks (ANN) genannt,  ist einen Art Abbildung des menschlichen neuronalen Netzwerks. Diese Netzwerk besteht aus vernetzten Neuronen. In einem KNN ist ein Neuron nichts anderes als eine Element welche eine Nummer zwischen 0 - 1, speichert. In einem KNN gibt es drei Arten von Neuronen - Input Neuron, Hidden Neuron und Output Neuron.\n",
        "\n",
        "Beispiel ein Bild mit einer handschriftlichen Zahl 9 besteht aus 28 x 28 Pixel was somit 784 Neuronen darstellt. Jeder dieser Neuronen enthält einen Graustufenwert von 0.01 (Schwarz) zu 1.00 (Weiss). Diese Werte werden „Activation“ genannt. Dies 784 Neuronen stellen die erste Schicht unsere Netzwerks dar. Die letze Schicht wird nur noch aus 10 Neuronen welche die Nummern 0 - 9 darstellen. Die Schichten dazwischen werden „Hidden Layers“ genannt. Sämtliche Neuronen in den verschiedenen Layers sind miteinander verbunden. Die Verbindungen zwischen den Neuronen haben eigene Gewichtungen. Diese Gewichtung wird mit dem Input-Wert multipliziert. Hat also beispielsweise ein Input-Neuron einen Wert von 0,7 und die Gewichtung der Verbindung einen Wert von 2 werden diese zwei Werte multipliziert und ergeben einen Wert von 1.4, welcher an das nächste Neuron weitergegeben werden. Würden wir aber nun immer so rechnen, würde das Resultat immer durch null gehen. Damit wir das verhindern können benutzen wir den Bias. Der Bias ist nichts anderes als der y-Intercept in der linearen Algebra mit dem Unterschied, dass er nur 1 sein kann. Nehmen wir nun also den Bias noch dazu dann bekommen wir folgenden Formel f(x) = mx + b.\n",
        "\n",
        "In einem künstlich neuronalen Netzwerk bestimmen die Aktivitäten in einem Layer die Aktivitäten im nächsten Layer. Die grosse Frage ist dabei WIE Aktivitäten in einem Layer die des nächsten beeinflusst. Grundsätzlich funktioniert es in der gleichen Art wie das biologische Vorbild. Feuert ein Neuron beeinflusst es ein anderes Neuron.\n",
        "\n",
        "Hier eine Abbildung eines typischen KNN:\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/MultiLayerNeuralNetworkBigger_english.png/880px-MultiLayerNeuralNetworkBigger_english.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rLA0iCN7kHVC"
      },
      "source": [
        "###Anatomie eines KNN###\n",
        "\n",
        "Nachfolgenden werden wir die verschiedenen Bestandteile (inkl. Math) eines KNN erläutern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g9C6y4I-skn8"
      },
      "source": [
        "**Künstliches Neuron (Unit)**\n",
        "\n",
        "Ein künstliches Neuron stellt ein vereinfachtes Modell eines biologischen Neurons dar.  Ein künstliches Neuron wird auch oft McCulloch-Pittsburgh-Zelle genannt. Bei der Definition des künstlichen Neurons wurden die wesentlichen Eigenschaften eines biologischen Neurons dargestellt:\n",
        "\n",
        "* Synapsen der Nervenzellen = Addition gewichteter EIngaben\n",
        "* Aktivierung Zellkerns = Aktivierungsfunktion mit Schwellwerten\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4oZb3hJ4YOIv"
      },
      "source": [
        "**Perzeptron**\n",
        "\n",
        "Ausgehen von dieser Idee hat Frank Rosenblatt 1957 das Perzepton entwickelt. Dieses Perzeptron stellt das einfachste KNN da es aus einer Eingabeschicht und einer Ausgabeschicht besteht:\n",
        "\n",
        "![alt text](https://www.statworx.com/wp-content/uploads/perceptron.png)\n",
        "\n",
        "Das Perzeptron besteht aus folgenden Teilen:\n",
        "\n",
        "1. **Input-Layer**. Der Input-Layer beinhaltet einerseits die Daten (x) und eine Bias-Unit (1)\n",
        "2. **Wichtung (w)**. Die Gewichtung wird pro Verbindung zwischen dem Input und Output dargestellt und beinhaltet irgend einen Wert. \n",
        "2. **Output-Layer** Die Ausgabeschicht besteht im wesentlichen aus der **Linear Threshold Unit (LTU)** welche folgende Funktionen beinhaltet: \n",
        " * **Übertragungsfunktion**. Diese Funktion (∑) berechnet die Wichtung mal Input plus Bias:\n",
        " \n",
        " $\\displaystyle \\sum_{i=1}^{n} =  (w_1 * _1 + w_2 * x_2 + w_n * x_n) + Bias$\n",
        "\n",
        "3. **Aktivierungsfunktion (ρ)**. Diese Funktion bestimmt die Ausgabe des Neurons. Sie wird durch die Netzeingabe und den Schwellwert beeinflusst. -> Siehe Activation Functions\n",
        "4. **Schwellwert (θj)**. Das Addieren eines Schwellwerts zur Netzeingabe verschiebt die gewichtete Eingabe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yL3SY6Urqo9O"
      },
      "source": [
        "**Input Layer**\n",
        "\n",
        "Die Daten werden über den Input Layer in das KNN übergeben. Im ersten Layer finden somit noch keine eigentlichen mathematischen Operationen statt und es gibt dort auch keine eigentlichen Knoten wie es in manchen Darstellungen suggeriert wird. Die Daten werden mittels eines 1D-Arrays in die Input-Schicht "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oV4X7vRdIByf"
      },
      "source": [
        "###Automated Machine Learning###\n",
        "\n",
        "**Tutorial:** [lntroduction: Automated Machine Learning](https://colab.research.google.com/drive/1CIVn-GoOyY3H2_Bv8z09mkNRokQ9jlJ-#scrollTo=rbiKVIwfHxxS)\n",
        "\n",
        "**Weitere Infos:** [Home TPOT](https://epistasislab.github.io/tpot/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lV1ZkNzThJ1L",
        "colab": {}
      },
      "source": [
        "# Install Upgrade Scikit-Learn 0.20 for AutoSklearn on the server\n",
        "!pip install --upgrade scikit-learn\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gbXgkMTEJhIs",
        "colab": {}
      },
      "source": [
        "# Install tpot on the server\n",
        "!pip install tpot\n",
        "\n",
        "# Install AutoSklearn on the server\n",
        "!pip install auto-sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eeXST0s0eNCK",
        "colab": {}
      },
      "source": [
        "# pandas and numpy for data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import the tpot regressor\n",
        "from tpot import TPOTRegressor\n",
        "\n",
        "import autosklearn.classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELtCrIcWJ7DP",
        "colab_type": "text"
      },
      "source": [
        "##Modelltraining (Train Model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTeWxo86J-Di",
        "colab_type": "text"
      },
      "source": [
        "##Modelltesting (Test Model)##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxxlwT_XKEKZ",
        "colab_type": "text"
      },
      "source": [
        "##Modelloptimierung (Tune Model)##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhDGAgE9KIYJ",
        "colab_type": "text"
      },
      "source": [
        "##Modellproduktivsetzung (Prediction)##"
      ]
    }
  ]
}
